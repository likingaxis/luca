---
{"dg-publish":true,"permalink":"/anno-2/probabilita/cap-1-riassunto-mega-supremo-per-capire-prob/"}
---

## ğŸ§© 1. Introduzione: perchÃ© usare la probabilitÃ  negli algoritmi

Il capitolo inizia con unâ€™idea importante:
ğŸ‘‰ **La probabilitÃ  serve per controllare lâ€™incertezza e lâ€™efficienza.**

Gli autori mostrano che in informatica possiamo usare la casualitÃ  non solo per modellare fenomeni incerti (come errori o dati rumorosi), ma anche per **progettare algoritmi piÃ¹ semplici e veloci**.  
Questi sono detti **algoritmi randomizzati**.

---

### ğŸ”¹ **Primo esempio: verificare unâ€™identitÃ  polinomiale**

Supponiamo di voler verificare se due polinomi sono uguali:

$$
F(x) = G(x)
$$

In teoria, potresti espandere tutto e confrontare i coefficienti â€” ma questo Ã¨ lento.  
Invece, si puÃ² fare cosÃ¬:

1. Scegli un numero casuale $r$ (per esempio tra $1$ e $100d$, dove $d$ Ã¨ il grado massimo dei polinomi).  
2. Calcola $F(r)$ e $G(r)$.  
3. Se i risultati sono diversi â†’ i polinomi **non sono uguali**.  
4. Se sono uguali â†’ **probabilmente** lo sono davvero.

---

### ğŸ¤” **PerchÃ© â€œprobabilmenteâ€?**

Ãˆ possibile (anche se raro) che due polinomi diversi assumano lo stesso valore per un certo $r$.  
Ma questo puÃ² accadere **al massimo in $d$ punti diversi**, dove $d$ Ã¨ il grado del polinomio.  

Quindi la probabilitÃ  di errore Ã¨ al piÃ¹:

$$
\frac{d}{100d} = \frac{1}{100}
$$

ğŸ‘‰ Con una sola prova hai solo **lâ€™1% di errore**.  
Ripetendo la prova piÃ¹ volte con numeri casuali diversi, la **probabilitÃ  di errore decresce esponenzialmente**.

ğŸ“˜ Ãˆ un esempio classico di **algoritmo randomizzato con errore controllato**:
non garantisce sempre la correttezza, ma Ã¨ **molto veloce** e **quasi sempre giusto**.

---

## ğŸ² 1.2 â€“ Axiomi di probabilitÃ 

Dopo lâ€™esempio pratico, gli autori formalizzano i concetti matematici su cui si basa la probabilitÃ .

---

### ğŸ”¸ **Lo spazio di probabilitÃ **

Un esperimento casuale (come â€œscegli un numero tra $1$ e $100d$â€) Ã¨ descritto da:

- **Spazio campionario** $\Omega$:  
  insieme di tutti i possibili risultati.  
  â†’ Nellâ€™esempio: $\Omega = \{1, 2, \dots, 100d\}$.

- **Eventi:** sottoinsiemi di $\Omega$.  
  â†’ Esempio: â€œil numero scelto Ã¨ pariâ€.

- **Funzione di probabilitÃ ** $\Pr$:  
  assegna a ogni evento un numero tra $0$ e $1$, con tre proprietÃ  fondamentali (gli **assiomi di Kolmogorov**):

  1. $0 \leq \Pr(E) \leq 1$
  2. $\Pr(\Omega) = 1$
  3. Se due eventi non si sovrappongono,  
     $$
     \Pr(E_1 \cup E_2) = \Pr(E_1) + \Pr(E_2)
     $$

Per eventi qualsiasi (anche sovrapposti) vale:

$$
\Pr(E_1 \cup E_2) = \Pr(E_1) + \Pr(E_2) - \Pr(E_1 \cap E_2)
$$

Questo Ã¨ utile per **combinare piÃ¹ probabilitÃ **.

---

### âš–ï¸ **La â€œUnion Boundâ€**

Una regola semplice ma potentissima:

$$
\Pr(E_1 \cup E_2 \cup \dots \cup E_n) \leq \Pr(E_1) + \Pr(E_2) + \dots + \Pr(E_n)
$$

In parole povere:
> La probabilitÃ  che accada almeno uno degli eventi Ã¨ al massimo la **somma delle loro probabilitÃ **.

Ãˆ uno strumento fondamentale per ottenere **stime superiori (upper bounds)** in probabilitÃ  e analisi di algoritmi.

---

## ğŸ§® **Ritorno allâ€™algoritmo dei polinomi**

Applichiamo ora le definizioni allâ€™esempio iniziale:

- Lo spazio campionario Ã¨ $\{1, 2, \dots, 100d\}$.  
- Ogni numero ha probabilitÃ  $\frac{1}{100d}$.

Lâ€™evento â€œlâ€™algoritmo sbagliaâ€ Ã¨ che $r$ sia una radice del polinomio $F(x) - G(x)$.  
PoichÃ© ci sono al massimo $d$ radici:

$$
\Pr(\text{errore}) \leq \frac{d}{100d} = \frac{1}{100}
$$

Se si ripete lâ€™esperimento $k$ volte (scegliendo $r$ indipendenti):

$$
\Pr(\text{errore in tutte le prove}) \leq \left(\frac{1}{100}\right)^k
$$

ğŸ‘‰ Quindi lâ€™errore scende **esponenzialmente** con il numero di ripetizioni.

---

## ğŸ§  **Indipendenza e probabilitÃ  condizionata**

Per comprendere meglio i calcoli precedenti, servono due concetti base.

### ğŸ”¹ **Eventi indipendenti**

Due eventi $E$ e $F$ sono **indipendenti** se:

$$
\Pr(E \cap F) = \Pr(E) \Pr(F)
$$

cioÃ¨: sapere che uno accade **non cambia** la probabilitÃ  dellâ€™altro.

---

### ğŸ”¹ **ProbabilitÃ  condizionata**

Definita come:

$$
\Pr(E \mid F) = \frac{\Pr(E \cap F)}{\Pr(F)}
$$

cioÃ¨: la probabilitÃ  che accada $E$ **sapendo che** $F$ Ã¨ accaduto.

---

### ğŸ” **Applicazione allâ€™algoritmo randomizzato**

- **Con sostituzione:** ogni scelta Ã¨ indipendente â‡’ si puÃ² **moltiplicare le probabilitÃ **.  
- **Senza sostituzione:** le prove sono collegate â‡’ si usa la **probabilitÃ  condizionata**.

---
## ğŸ§© 1.3 â€“ Verifying Matrix Multiplication  
**Verificare una moltiplicazione di matrici in modo probabilistico**

Questa sezione mostra unâ€™idea potente: usare la **probabilitÃ ** per rendere **efficienti verifiche molto costose**.

---

### ğŸ”¹ **Il problema**

Abbiamo tre matrici quadrate $A$, $B$ e $C$, ciascuna di dimensione $n \times n$.  
Vogliamo verificare se:

$$
A \times B = C
$$

Il metodo diretto â€” calcolare $A \times B$ e confrontarlo con $C$ â€” richiede **$O(n^3)$** operazioni,  
cioÃ¨ milioni di moltiplicazioni per matrici grandi.  
Troppo lento, soprattutto se vogliamo solo **controllare** che il risultato sia corretto.

---

### ğŸ”¹ **Lâ€™idea randomizzata**

Invece di rifare tutta la moltiplicazione, scegli un **vettore casuale** $r$ di dimensione $n \times 1$  
(ad esempio con valori 0 o 1 scelti a caso).

Poi calcola:

$$
A(Br) \quad \text{e} \quad Cr
$$

e confrontali:

- Se $A(Br) = Cr$, **probabilmente** $A \times B = C$.
- Se $A(Br) \neq Cr$, **sicuramente** $A \times B \neq C$.

---

### ğŸ”¹ **Cosa significano $A(Br)$ e $Cr$**

$Br$ significa â€œmoltiplica la matrice $B$ per il vettore casuale $r$â€:  
ottieni un nuovo vettore, combinazione delle colonne di $B$.

Poi moltiplichi $A$ per quel risultato: $A(Br)$.  
Questo equivale ad applicare prima $B$, poi $A$ al vettore $r$.

$Cr$ Ã¨ invece la moltiplicazione della matrice $C$ per lo stesso vettore $r$.

â¡ï¸ In sintesi, il test confronta due vettori:
- il risultato di applicare $A$ e $B$ in cascata,
- con quello di applicare direttamente $C$.

---

### ğŸ”¹ **PerchÃ© funziona**

Se $A \times B = C$, allora per ogni vettore $r$ vale:

$$
A(Br) = (AB)r = Cr
$$

Quindi il test sarÃ  sempre vero.

Se invece $A \times B \neq C$, definiamo:

$$
D = A \times B - C
$$

PoichÃ© $D \neq 0$, vogliamo verificare se:

$$
Dr = 0
$$

cioÃ¨ se la combinazione delle colonne di $D$ data da $r$ restituisce il vettore nullo.

Se $D \neq 0$, quasi sempre ci sarÃ  almeno un vettore $r$ per cui $Dr \neq 0$.  
Solo in casi rari (con probabilitÃ  $\le \frac{1}{2}$) potremmo scegliere casualmente un vettore che â€œnascondeâ€ lâ€™errore.

Ripetendo il test piÃ¹ volte con vettori diversi, la probabilitÃ  di errore scende **esponenzialmente**:

$$
\Pr(\text{errore dopo $k$ prove}) \le \left(\frac{1}{2}\right)^k
$$

---

### ğŸ”¹ **Vantaggi**

| Metodo | ComplessitÃ  | Accuratezza |
|--------|--------------|-------------|
| Classico | $O(n^3)$ | Esatta |
| Probabilistico | $O(n^2)$ | Quasi certa (errore â‰¤ $(1/2)^k$) |

ğŸ‘‰ Ãˆ un esempio di **algoritmo Monte Carlo**:  
usa la casualitÃ  per ottenere risultati piÃ¹ rapidi, accettando un piccolissimo margine dâ€™incertezza.

---

## ğŸ² 1.4 â€“ Conditional Probability and Independence  
**ProbabilitÃ  condizionata e indipendenza**

Gli autori introducono ora formalmente i concetti di **probabilitÃ  condizionata** e **indipendenza**, giÃ  usati negli esempi precedenti.

---

### ğŸ”¹ **ProbabilitÃ  condizionata**

$$
\Pr(E \mid F) = \frac{\Pr(E \cap F)}{\Pr(F)}
$$

Si legge:  
> â€œLa probabilitÃ  che accada $E$, **dato che** Ã¨ accaduto $F$.â€

---

#### ğŸ“˜ Esempio

In un mazzo di 52 carte:

- $E$: â€œÃ¨ un reâ€ â†’ 4 carte  
- $F$: â€œÃ¨ una figuraâ€ (re, donna o fante) â†’ 12 carte  

PoichÃ© $E \cap F = E$:

$$
\Pr(E \mid F) = \frac{\Pr(E)}{\Pr(F)} = \frac{4/52}{12/52} = \frac{1}{3}
$$

Quindi, sapendo che Ã¨ uscita una figura, la probabilitÃ  che sia un re Ã¨ **1/3**.

---

### ğŸ”¹ **Regola del prodotto**

Dalla definizione di probabilitÃ  condizionata:

$$
\Pr(E \cap F) = \Pr(F) \times \Pr(E \mid F)
$$

oppure anche:

$$
\Pr(E \cap F) = \Pr(E) \times \Pr(F \mid E)
$$

Questa Ã¨ la base per molti calcoli, incluso il **Teorema di Bayes**.

---

### ğŸ”¹ **Indipendenza**

Due eventi $E$ e $F$ sono **indipendenti** se:

$$
\Pr(E \cap F) = \Pr(E) \Pr(F)
$$

cioÃ¨ sapere che uno accade **non cambia** la probabilitÃ  dellâ€™altro.

E quindi anche:

$$
\Pr(E \mid F) = \Pr(E)
$$

---

#### ğŸ² Esempio

Lancio due monete:

- $E$: â€œla prima moneta Ã¨ testaâ€  
- $F$: â€œla seconda moneta Ã¨ testaâ€

â†’ Sono **indipendenti**, perchÃ© lâ€™esito della prima non influisce sulla seconda.

---

## ğŸ§  1.5 â€“ Naive Bayesian Classifier  
**Unâ€™applicazione della probabilitÃ  condizionata alla classificazione**

Questa sezione fa da ponte tra **probabilitÃ ** e **intelligenza artificiale**, mostrando come usare il **Teorema di Bayes** per classificare oggetti o dati in categorie.

---

### ğŸ”¹ **Il modello probabilistico**

Vogliamo stimare:

$$
\Pr(C \mid x) = \frac{\Pr(C) \times \Pr(x \mid C)}{\Pr(x)}
$$

dove:

- $C$ = categoria (es. â€œspamâ€ o â€œnon spamâ€)  
- $x$ = insieme delle caratteristiche osservate (es. parole di unâ€™email)

ğŸ‘‰ Lâ€™obiettivo Ã¨ scegliere la categoria $C$ che **massimizza** $\Pr(C \mid x)$.

---

### ğŸ”¹ **Cosa significa categoria**

Una categoria Ã¨ una **classe o etichetta** che identifica il tipo di oggetto:

| Oggetto | Categorie possibili |
|----------|---------------------|
| Email | Spam / Non spam |
| Immagine | Gatto / Cane |
| Recensione | Positiva / Negativa |

Il sistema osserva un insieme di caratteristiche $x$ (es. le parole di un testo)  
e cerca la categoria piÃ¹ probabile.

---

### ğŸ”¹ **Cosa significa $\Pr(x \mid C)$**

> â€œLa probabilitÃ  di osservare le caratteristiche $x$, sapendo che lâ€™oggetto appartiene alla categoria $C$.â€

Esempio:

- $\Pr(x \mid \text{spam})$ = probabilitÃ  che unâ€™email di spam contenga â€œlotteriaâ€ e â€œpremioâ€  
- $\Pr(x \mid \text{non spam})$ = probabilitÃ  che unâ€™email normale contenga le stesse parole

---

### ğŸ”¹ **Lâ€™assunzione â€œnaiveâ€**

Nella realtÃ  le caratteristiche non sono indipendenti (â€œlotteriaâ€ e â€œpremioâ€ compaiono spesso insieme).  
Ma per semplificare i calcoli si assume che, dato $C$, **siano indipendenti**.

$$
\Pr(x \mid C) = \Pr(x_1, x_2, \dots, x_n \mid C) = \prod_i \Pr(x_i \mid C)
$$

Il simbolo $\prod_i$ indica il prodotto di tutte le probabilitÃ  individuali $\Pr(x_i \mid C)$.

Esempio:

$$
\Pr(x \mid \text{spam}) = 0.8 \times 0.7 \times 0.4 = 0.224
$$

---

### ğŸ”¹ **Il simbolo â€œâˆâ€ (proporzionale a)**

Il teorema di Bayes completo Ã¨:

$$
\Pr(C \mid x) = \frac{\Pr(C) \Pr(x \mid C)}{\Pr(x)}
$$

PoichÃ© $\Pr(x)$ Ã¨ uguale per tutte le categorie (Ã¨ lo stesso messaggio), possiamo ignorarlo e scrivere:

$$
\Pr(C \mid x) \propto \Pr(C) \times \Pr(x \mid C)
$$

Il simbolo â€œâˆâ€ significa â€œproporzionale aâ€,  
cioÃ¨ differisce solo per un **fattore costante** $\frac{1}{\Pr(x)}$.

---

### ğŸ”¹ **Esempio numerico completo**

| Parola | $\Pr(\text{parola} \mid \text{spam})$ | $\Pr(\text{parola} \mid \text{non spam})$ |
|:-------|:-------------------------------------:|:------------------------------------------:|
| â€œlotteriaâ€ | 0.8 | 0.1 |
| â€œpremioâ€ | 0.7 | 0.05 |

E sappiamo che:

$$
\Pr(\text{spam}) = 0.4, \quad \Pr(\text{non spam}) = 0.6
$$

Allora:

$$
\Pr(\text{spam} \mid x) \propto 0.4 \times (0.8 \times 0.7) = 0.224
$$

$$
\Pr(\text{non spam} \mid x) \propto 0.6 \times (0.1 \times 0.05) = 0.003
$$

PoichÃ© $0.224 \gg 0.003$, il messaggio Ã¨ classificato come **spam**.

---

### ğŸ”¹ **Intuizione finale**

- $\Pr(C)$ â†’ quanto Ã¨ comune quella categoria (â€œquanto spesso ricevo spamâ€).  
- $\Pr(x \mid C)$ â†’ quanto il contenuto Ã¨ tipico di quella categoria.  
- $\Pr(C \mid x)$ â†’ quanto Ã¨ probabile che il messaggio appartenga a quella categoria, **dato il contenuto**.

PoichÃ© $\Pr(x)$ Ã¨ costante, possiamo semplicemente scegliere la categoria con:

$$
\Pr(C) \Pr(x \mid C) \text{ massimo.}
$$

âœ… In pratica, Ã¨ cosÃ¬ che funzionano i classificatori **Naive Bayes** negli algoritmi di machine learning.
