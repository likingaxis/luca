---
{"dg-publish":true,"permalink":"/anno-3/intelligenza-artificiale/esercitazioni/esempio-esercizio/"}
---

## simulatore d'ambiente

```scss
function RUN-EVAL-ENVIR (state, UPDATE-FN, CV, PERFORMANCE-FN) returns CE
  local variables  CE   % inizialmente 0

  repeat
      (NWpos, Dpos, PRpos, EXpos) ‚Üê GET-PERCEPT(CV, state)

      Action ‚Üê CV-PROG(NWpos, Dpos, PRpos, EXpos)

      state  ‚Üê UPDATE-FN(state, Action)

      CE     ‚Üê PERFORMANCE-FN(state, CE)

  until TERMINATION(state)

  return CE

```
---
üîπ `GET-PERCEPT(CV, state)`
Legge dallo **stato del mondo** le informazioni che l‚Äôagente pu√≤ osservare  
(es. posizione cavaliere, principessa, drago, uscita)  
e le passa come **percezione** al programma agente.
üîπ `CV-PROG(...)`
√à il **programma dell‚Äôagente** (il cavaliere):  
prende la percezione e decide **una sola azione** da compiere in questo ciclo  
(es. N, S, E, O).
üîπ `UPDATE-FN(state, Action)`
Aggiorna lo **stato dell‚Äôambiente** applicando l‚Äôazione:  
sposta il cavaliere, segna se ha preso la principessa, controlla se √® finito sul drago, ecc.
üîπ `PERFORMANCE-FN(state, CE)`
Aggiorna il **costo / punteggio cumulativo** in base al nuovo stato  
(es. +1 per ogni passo, penalit√† se entra in zona pericolosa).
üîπ `TERMINATION(state)`
Controlla se la simulazione deve finire  
(es. cavaliere √® uscito con la principessa, oppure √® morto, oppure finito il tempo).
üîπ `return CE`
Restituisce il **costo totale** (o score) del comportamento dell‚Äôagente nell‚Äôambiente.

## ‚öôÔ∏è PROGRAMMA AGENTE

```scss
function CV-PROGRAM(percept) returns action
    persistent:
        plan      % sequenza di azioni
        state     % stato interno stimato (posizioni ecc.)
        goal      % goal corrente
	    problem 

    if (CVpos = PRpos) and (hasPrincess = false) then
        hasPrincess ‚Üê true
        goal ‚Üê "torna_all_uscita"
        plan ‚Üê EMPTY          % forza una nuova pianificazione
    if (hasPrincess = true) and (CVpos = EXpos) then
        return exit       
    if plan = EMPTY then
        if hasPrincess = false then
            goal ‚Üê PRpos      % primo goal: raggiungi principessa
        else
            goal ‚Üê EXpos      % secondo goal: torna all‚Äôuscita
        problem ‚Üê FORMULATE-PROBLEM(perception, goal)
        plan    ‚Üê SEARCH(problem)   % es. A* su MAZE evitando Dpos
        if plan = failure then
            return null-action
    end if
    action ‚Üê FIRST(plan)
    plan   ‚Üê REST(plan)
    return action
```

## üîç Significato riga per riga
**1. `FORMULATE-PROBLEM(state, goal)`**
Costruisce il _problema di ricerca_ da dare all‚Äôalgoritmo di pianificazione.  
Contiene:
- stato iniziale (posizione attuale)
- goal da raggiungere
- azioni disponibili
- modello di transizione
- costi
- euristica
Serve per dire al planner **‚Äúda qui voglio arrivare l√¨, questo √® il mondo‚Äù**.
 **2. `SEARCH(problem)`**
Esegue l‚Äôalgoritmo di ricerca (es. A*).  
Input: problema formale.  
Output: **plan**, cio√® una lista di azioni ottima per raggiungere il goal.  
Se non trova soluzione ‚Üí `failure`.
Serve per **calcolare il percorso**.
 **3. `FIRST(plan)`**
Restituisce la **prima azione** della lista `plan`.
Serve per **decidere cosa fare ora**.
 **4. `REST(plan)`**
Rimuove la prima azione dalla lista, lasciando il resto.
Serve per **avanzare nel piano** passo dopo passo.
**5. Logica di cambio goal**
Nel codice ci sono due condizioni speciali:
- **se raggiunge la principessa** ‚áí cambia goal all‚Äôuscita e svuota il piano
- **se raggiunge l‚Äôuscita** ‚áí termina (`exit`)
Serve per gestire **le due fasi** del problema.
**6. `return action`**
Restituisce l‚Äôazione da eseguire **in questo ciclo**.

### A*

```scss
function A* (problem) returns a solution or failure
nodo <- nodo con stato = problem.initialstate
frontiera <- coda di priorit√† ordinata in base a f(n) con all'inizio solo nodo "nodo"
esplorati <- insieme dei nodi esplorati inizialmente vuoto
loop do
    if frontiera is empty? then return failure
    nodo <- POP(frontiera)
    if problem.GOALTEST(nodo.state) then return SOLUTION(nodo)
    add nodo.state to esplorati
    for each action in problem.ACTIONS(nodo.state) do
        child <- CHILD-NODE(problem, nodo, action)
        if child.state non in frontiera or esplorati then
            frontiera <- INSERT(child.state)
        else if child.state is in frontiera con f(n) pi√π alto allora
            replace that frontier node with child
```

üîπ `problem.initialstate`
Stato iniziale del problema: da dove parte la ricerca.
üîπ `frontiera` (priority queue su f(n))
Insieme dei nodi ‚Äúda esplorare dopo‚Äù, ordinati per **f(n) = g(n) + h(n)**.  
`POP(frontiera)` prende il nodo con f pi√π piccolo.
üîπ `esplorati`
Insieme degli **stati gi√† espansi**: serve per non riesplorare gli stessi stati.
üîπ `problem.GOALTEST(nodo.state)`
Test logico: controlla se lo stato del nodo corrente √® un **goal**.  
Se s√¨ ‚Üí ricostruisce e ritorna la soluzione (`SOLUTION(nodo)` seguendo i parent).
üîπ `problem.ACTIONS(nodo.state)`
Restituisce l‚Äôinsieme delle **azioni applicabili** in quello stato (es. {su, gi√π, dx, sx}).
üîπ `CHILD-NODE(problem, nodo, action)`
Crea il **nodo figlio**:
- calcola il nuovo stato con il modello di transizione
- aggiorna g(n), h(n), f(n)
- mette il `parent` = `nodo` e l‚Äôazione usata.
üîπ Test su `frontiera` / `esplorati`
- se lo stato del figlio non √® mai stato visto ‚Üí lo inserisce in `frontiera`
- se √® gi√† in `frontiera` ma con f peggiore ‚Üí lo **sostituisce** con la versione migliore