---
{"dg-publish":true,"permalink":"/anno-3/intelligenza-artificiale/primo-esonero/ia-lez-4/"}
---

# Ricerca esaustiva
Nella ricerca **non informata**, come BFS o UC, lâ€™agente esplora tutto lo spazio degli stati â€œalla ciecaâ€, senza sapere quale strada lo avvicina davvero alla soluzione.  
	â†’ Questo Ã¨ **impraticabile** quando il numero di stati cresce in modo **esponenziale**.

### Lâ€™idea base dellâ€™euristica
Una **euristica** Ã¨ una _stima intelligente_ della distanza (o del costo) che manca per raggiungere lâ€™obiettivo.
- Deriva da **esperienza o conoscenza del dominio**
- Non elimina la ricerca, ma **la rende piÃ¹ efficiente**, perchÃ© esplora prima i nodi piÃ¹ promettenti.
- Di solito non garantisce lâ€™ottimo assoluto, ma una **buona soluzione in tempi accettabili**.

>[!tip] Conoscenza Euristica indica una "**scelta oculata**" o un'intuizione che aiuta a risolvere il problema



### Funzione di valutazione euristica
La conoscenza euristica si formalizza in una **funzione**:
$$f: n \rightarrow \mathbb{R}
$$
Questa associa a ogni **nodo n** (che rappresenta uno stato) un **valore numerico** che misura â€œquanto sembra promettenteâ€ quel nodo.

In pratica:
- $f(n)$ Ã¨ un **numero reale**;
- piÃ¹ Ã¨ **basso**, piÃ¹ il nodo Ã¨ â€œvicinoâ€ o â€œeconomicoâ€ rispetto al goal;
- si calcola **a partire dallo stato del nodo (`n.Stato`)**, non dalla sua storia.


---

##### Alcuni esempi di euristica
![Pasted image 20251025111157.png](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025111157.png)


#### Algoritmo di ricerca Best-first
#### Best-First classico
![Pasted image 20251025113910.png](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025113910.png)
![Pasted image 20251025111757.png](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025111757.png)

- **g(n)** = costo reale del cammino dallâ€™inizio al nodo n.
- Nessuna euristica: usiamo solo il costo accumulato finora.
In questo caso, la â€œBest-Firstâ€ **coincide esattamente con la Ricerca di Costo Uniforme (Uniform-Cost Search)**.

Infatti:
- lâ€™algoritmo sceglie sempre il nodo con il **costo cumulativo minore** (`lowest-cost node in frontier`),
- e continua ad espandere fino a trovare il goal con costo minimo.

#### Best-First Greedy
![Pasted image 20251025112102.png](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025112102.png)

![Pasted image 20251025111949.png](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025111949.png)
- **h(n)** = stima euristica del costo _dal nodo n al goal_ (quanto â€œmancaâ€).
- Lâ€™algoritmo ignora completamente il costo giÃ  speso **(g(n))**, guarda solo _chi sembra piÃ¹ vicino alla meta_.

Ãˆ come dire:
> â€œSeguo sempre la direzione che sembra piÃ¹ promettente, anche se magari sto facendo un giro piÃ¹ lungo.â€

**Vantaggi:**
- molto piÃ¹ veloce, perchÃ© guida subito verso il goal.  
**Svantaggi:**
- non Ã¨ ottimale (puÃ² trovare percorsi piÃ¹ costosi).   
- non Ã¨ garantito che sia completa se ci sono cicli o stime sbagliate.

###### **G(N) vs H(N): **
###### **1.Â g(n)Â (Passato)**
- CostoÂ **reale e cumulativo**Â dall'inizio al nodoÂ n.
- **NON Ã¨ euristica.**
- Conosce il costoÂ **giÃ  sostenuto**Â (certo, non stima)
###### **2.Â h(n)Â (Futuro)**
- **Stima euristica**Â del costo che manca daÂ nÂ al goal.
- **Ãˆ euristica.**
- **Non conosce**Â il costo reale futuro (Ã¨ una "scommessa").
- **Motivo:**Â Ambiente complesso, mancanza info, velocitÃ .
- **Esempio:**Â Distanza in linea d'aria (stima approssimativa).
###### **3. Conclusione**
- AvereÂ g(n)Â **NON rende euristico.**Â L'euristica Ã¨ data daÂ h(n).
---
# A* search
L'idea principale Ã¨ cercare un equilibrio tra:
- **arrivare al goal** (come Greedy),
- **risparmiare sul costo fatto finora** (come Uniform Cost),  
Quindi l'alg, vuole **trovare il percorso totale piÃ¹ economico possibile**, stimando il costo complessivo.

### La funzione di valutazione
A* usa:$$
f(n)=g(n)+h(n)$$
dove:
- **g(n)** = costo _reale_ per arrivare fino al nodo `n` (giÃ  percorso);
- **h(n)** = stima _euristica_ del costo rimanente per arrivare al goal;
- quindi **f(n)** = stima del costo _totale_ del cammino passando per n.

In altre parole:
> A* valuta ogni nodo come â€œquanto ho speso finora + quanto (credo) manchi ancoraâ€.


### Come funziona in pratica
Lâ€™algoritmo mantiene una **coda con prioritÃ ** ordinata per $f(n)$:
1. sceglie il nodo con f(n) piÃ¹ basso (il â€œpiÃ¹ promettenteâ€ considerando costi reali + stimati);
2. lo espande, generando i successori;
3. aggiorna la frontiera con i nuovi nodi e i loro f(n).

Continua finchÃ© trova un nodo obiettivo.
### ProprietÃ  della funzione $h(n)$
Per garantire che A* sia **completo e ottimale**, servono due condizioni:
1. **h(n) â‰¥ 0** â€” i costi stimati non possono essere negativi (ovvio ma importante);
2. **h(goal) = 0** â€” al goal il costo residuo Ã¨ nullo.

Se poi lâ€™euristica Ã¨ **ammissibile** (cioÃ¨ non sovrastima mai il vero costo), A* trova **sempre la soluzione ottima**.

### Relazioni con altri algoritmi
A* Ã¨ una **famiglia di algoritmi â€œAâ€**, e in base a come scegli h(n) o g(n) ottieni casi particolari:

| Caso     | Condizione           | Diventaâ€¦                     | Significato                                      |
| -------- | -------------------- | ---------------------------- | ------------------------------------------------ |
| h(n) = 0 | â‡’ f(n) = g(n)        | **Uniform Cost Search (UC)** | ignora la stima futura â†’ esplora per costo reale |
| g(n) = 0 | â‡’ f(n) = h(n)        | **Greedy Best-First Search** | ignora il costo fatto â†’ segue la stima del goal  |
| entrambi | â‡’ f(n) = g(n) + h(n) | **A***                       | bilancia costo reale + stimato                   |

- quando la h ha certe caratteristiche diventa A* altrimenti Ã¨ solo A
#### Esempi algoritmi A
![Pasted image 20251025114716.png](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025114716.png)
![Pasted image 20251025114733.png](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025114733.png)

## A Ã¨  completo
>[!lemma] Teorema: L'algoritmo `A` con la condizione $$g(n) \ge d(n) \cdot \epsilon$$Ã¨ completo. 
>Con
>- **g(n)** = costo accumulato per arrivare al nodo n (cioÃ¨ quanto abbiamo speso fino a lÃ¬);
>- **d(n)** = profonditÃ  del nodo (cioÃ¨ quanti passi/azioni abbiamo fatto);   
>- **Îµ** = il **costo minimo possibile** di unâ€™azione (quindi un numero positivo).

Questa condizione dice:
> â€œOgni volta che lâ€™agente fa un passo, quel passo deve costare _almeno un poâ€™_, mai zero.â€
> In altre parole, **non possono esistere archi di costo zero o negativo**.

### PerchÃ© la condizione ci garantisce completezza?
Imponendo che ogni passo costi **almeno Îµ**, cioÃ¨ che il costo **cresca un minimo a ogni azione**, otteniamo due effetti:
1. **Ogni cammino ha un costo che aumenta** man mano che scendiamo di livello (non rimaniamo fermi a costo quasi 0).
2. **Esistono solo un numero finito di nodi con un costo minore del costo della soluzione** â†’ lâ€™algoritmo non puÃ² esplorare allâ€™infinito.

Questa Ã¨ la chiave della completezza.

### **completezza di A* (Dimostrazione Veloce) âœ…**
**ðŸ’¡ Idea di Base:**Â Se una soluzione esiste, A* la trova sempre!
###### **1. Partiamo da un Cammino Soluzione Esistente ðŸ›¤ï¸**
- Immaginiamo che esista un percorso dalloÂ STARTÂ alÂ GOAL.
- Chiamiamo questo percorso:Â $[n_0, n_1, ..., n_* ..., n_k = GOAL]$.
###### **2.Â n*Â un Nodo "Importante" sulla Frontiera ðŸŒŸ**
- Consideriamo un nodoÂ n*Â che fa parte di quel cammino soluzione e si trova nellaÂ frontieraÂ (la lista dei nodi da esplorare).
- **A* espanderÃ Â n*Â (prima o poi)!**
    - **PerchÃ©?**Â A* sceglie sempre il nodo con ilÂ f(n) = g(n) + h(n)Â piÃ¹ basso.
    - SeÂ h(n)Â Ã¨ "ammissibile" (cioÃ¨, non sovrastima mai il costo reale), allora iÂ f(n)Â dei nodi sul cammino soluzione non saranno mai "troppo alti".
    - Ci sonoÂ **solo un numero finito**Â di nodiÂ xÂ che hanno unÂ f(x)Â minore o uguale aÂ f(n*).
    - Questo significa che A* li esplorerÃ  tutti, e poi arriverÃ  anche aÂ n*. Non puÃ² "ignorare"Â n*Â all'infinito! â³
###### **3. A* Fa Progressi sul Cammino Soluzione âž¡ï¸**
- Quando A* espandeÂ n*, cosa succede?
    - I nodi vicini aÂ n*Â vengono aggiunti allaÂ frontiera.
    - **Cruciale:**Â Anche ilÂ **prossimo nodo**Â sul cammino soluzione originale verrÃ  aggiunto! âœ¨
###### **4. Iterazione Fino al Goal! ðŸŽ¯**
- Questo processo si ripete! Ogni volta che A* espande un nodo del cammino soluzione, aggiunge il successivo nodo del cammino soluzione alla frontiera.
- Quindi, A* continua a "muoversi" lungo il cammino soluzione.
- Alla fine, A* sarÃ  costretto a selezionare ed espandere anche il nodoÂ GOAL. ðŸŽ‰
###### **Algoritmo A*: L'Euristica "Ideale" vs. Quella Reale ðŸ§ **

###### **1. Funzione di Valutazione Ideale (Oracolo) ðŸ”®**
- Immaginiamo di avere unÂ **"oracolo"**Â che conosce tutto il futuro.
- **f*(n) = g*(n) + h*(n)**
    - **g*(n):**Â CostoÂ **minimo reale**Â dalÂ STARTÂ alla radiceÂ n. (Il meglio che si possa fare fino aÂ n).
    - **h*(n):**Â CostoÂ **minimo reale**Â daÂ nÂ alÂ GOAL. (Il meglio che si possa fare daÂ nÂ in poi).
    - **f*(n):**Â CostoÂ **minimo reale**Â del percorso totale dalÂ STARTÂ alÂ GOALÂ passando perÂ n.
- **Questo Ã¨ ciÃ² che vorremmo sapere sempre, ma non Ã¨ possibile!**Â ðŸ˜”
###### **2. La RealtÃ  dei Fatti (Normalmente) ðŸŒ**
- **g(n):**Â Il costo accumulato che calcoliamo Ã¨Â **sempre maggiore o uguale**Â al costo minimo reale:Â g(n) â‰¥ g*(n).
    - Non possiamo fare meglio del percorso ottimo, e il nostro percorso potrebbe essere sub-ottimale.
- **h(n):**Â La nostra euristica Ã¨ solo unaÂ **stima**Â diÂ h*(n).
    - Non possiamo conoscere il costo minimo reale futuro.
##### **Algoritmo A*: La Chiave Ã¨ l'Euristica Ammissibile ðŸ—ï¸**

###### **1. Definizione: Euristica Ammissibile (Consistent/Admissible Heuristic) âœ…**
- PerÂ **ogni nodoÂ n**, la nostra stimaÂ h(n)Â **NON deve MAI sovrastimare**Â il costo reale minimo per arrivare al goal:
    - âˆ€n. h(n) â‰¤ h*(n)Â âž¡ï¸Â hÂ Ã¨ unaÂ **sottostima**.
- **Esempio:**Â La distanza in linea d'aria. Ãˆ sempre minore o uguale al costo reale del percorso (non puoi fare meno strada che andare dritto!). ðŸ“
###### **2. Definizione: Algoritmo A* ðŸ¤–**
- Un algoritmo di ricerca che usa una funzioneÂ f(n) = g(n) + h(n), doveÂ h(n)Â Ã¨ unaÂ **funzione euristica ammissibile**.
###### **3. Teorema Importante: OttimalitÃ  di A* ðŸ†**
- Gli algoritmi A* (con euristica ammissibile) sonoÂ **sempre ottimali**.
- **Corollario (Conseguenza):**
    - **Best-First (BF)**Â eÂ **Uniform-Cost Search (UCS)**Â sono casi speciali di A* (o algoritmi ottimali) quandoÂ h(n)=0.
        - SeÂ h(n)=0, alloraÂ f(n) = g(n) + 0 = g(n). Questo Ã¨ esattamente come funziona Uniform-Cost Search! ðŸ’¡
###### **ProprietÃ  di A* ðŸŽ¯**
- **Completezza?**Â âœ…Â **SÃ¬.**
    - Trova sempre soluzione se esiste (se non infiniti nodi conÂ f(n) â‰¤ f(Goal)).
- **OttimalitÃ ?**Â âœ…Â **SÃ¬.**
    - Trova sempre il cammino meno costoso (con euristica ammissibile).
- **Tempo?**Â â±ï¸Â **Esponenziale.**
    - PuÃ² esplorare molti nodi nel caso peggiore.
- **Spazio?**Â ðŸ§ Â **Mantiene tutti i nodi in memoria.**
    - PuÃ² essere un problema per problemi grandi.
###### **Osservazioni su A* ðŸ¤”**
1. **Sottostima (h(n) â‰¤ h*(n))**
    - PuÃ² causare lavoro inutile (piÃ¹ nodi esplorati). ðŸš¶â€â™‚ï¸
    - **MA**Â garantisce di trovare il cammino migliore. ðŸ†
2. **Ruolo diÂ g(n)**
    - Fa abbandonare cammini che si approfondiscono troppo e diventano costosi. ðŸ›‘
3. **Sovrastima (h(n) > h*(n))**
    - PuÃ² far perdere la soluzione ottimale. ðŸš« (Per questo serve ammissibilitÃ ).
###### **OttimalitÃ  di A*: Contesto ðŸŒŸ**
- **Ricerca su Albero:**
    - L'**ammissibilitÃ **Â diÂ h(n)Â Ã¨ sufficiente per l'ottimalitÃ . âœ…
- **Ricerca su Grafo:**
    - Serve una proprietÃ Â **piÃ¹ forte**: laÂ **consistenza**Â (o monotonicitÃ ). ðŸ’ª
### Euristica consistente o monotÃ²na
La definizione formale di euristica consistente Ã¨ $$h(goal) = 0$$e $$\forall n, \ h(n) \le c(n,a,n') + h(n')$$dove
- $nâ€²$ Ã¨ un **successore** di $n$,
- $c(n,a,n^{â€²})$ Ã¨ il **costo reale** dellâ€™azione per andare da $n$ a $n^{â€²}$.

>[!tip]- Tradotto in parole semplici:
Unâ€™**euristica Ã¨ consistente** se la **stima del costo** da un nodo $n$ al goal **non Ã¨ mai maggiore** del costo reale per andare a un successore nâ€² **piÃ¹** la stima da nâ€² al goal.
>
In altre parole:
> â€œOgni passo che faccio verso il goal deve ridurre la stima h(n) di un valore **non superiore** al costo effettivo del passo.â€

### Cosa implica: $f(n) \le f(n^{'})$ (monotonia)
Dalla condizione sopra deriva automaticamente che:
$$f(n) = g(n) + h(n) \le g(n') + h(n') = f(n')$$

Questo significa che:
> lungo un cammino, il valore di $f(n)$ **non diminuisce mai**.

In altre parole:
- ogni volta che A* espande un nodo,
- i nodi successivi non potranno mai avere f piÃ¹ basso.

ðŸ‘‰ ecco perchÃ© si dice **euristica â€œmonotÃ²naâ€**:  
	f(n) **cresce o resta uguale**, ma **non scende mai**.
- una euristica invece si dice ammissibile se: 
Un'euristicaÂ h(n)Â Ã¨Â **ammissibile**Â se, per qualsiasi nodoÂ n, il suo valore Ã¨ sempre minore o uguale al costo realeÂ h*(n)Â per arrivare al goal.  

$h(n)â‰¤h^âˆ—(n)$

### PerchÃ© Ã¨ importante
Questa proprietÃ  ha due effetti pratici fondamentali per A*:
1. **Efficienza:**  
    Se h Ã¨ consistente, A* **non deve mai riespandere un nodo** giÃ  visitato.  
    (perchÃ© non potrÃ  mai trovare un percorso con f minore del precedente).
2. **Ordine perfetto:**  
    I nodi vengono espansi **in ordine non decrescente di f(n)**, cioÃ¨ dal piÃ¹ promettente al meno promettente.

ðŸ‘‰ Questo rende A* piÃ¹ semplice, piÃ¹ veloce e piÃ¹ sicuro nella gestione della frontiera.


## ProprietÃ  delle euristiche monÃ²tone
1. ***TEOREMA***: se un'euristica Ã¨ **monotona**, allora Ã¨ automaticamente ammissibile (non vale sempre il contrario!)
	
2. Esistono euristiche ammissibili che non sono monotone (ma sono rare)
	- nel senso che puÃ² esistere un'euristica che sottostimi (ammissibile), ma non rispetti la regola di monotonia
	
3. Le euristiche monotone garantiscono che la soluzione meno costosa venga trovata per prima
	Se h Ã¨ monotona, allora:
	- i valori di f(n) **non diminuiscono mai** lungo i cammini;
	- quindi A* esplora i nodi **in ordine crescente di f(n)** (dal piÃ¹ economico al piÃ¹ costoso).
	
	ðŸ‘‰ Risultato:
	- quando A* trova **una soluzione**, Ã¨ **sicuramente quella col costo minimo** â€”  
> perchÃ© nessun altro percorso meno costoso poteva avere un f piÃ¹ basso e venire scelto prima.


>[!tip]- Esempi euristiche ammissibile
>![Pasted image 20251025192755.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025192755.jpg)


## Bilancio su A*
![Pasted image 20251025192844.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025192844.jpg)


>[!problem] PROBLEMA DI A* -> occupa troppo a livello di memoria $(O(b^{d+1}))$ 


---

# Migliorare l'occupazione in memoria di A*

# Beam Search
Invece di tenere in memoria TUTTI I NODI, l'idea Ã¨ quella di ricordare solo i *k nodi piÃ¹ promettenti*, dove `k` Ã¨ detto **ampiezza del raggio (beam)**.

>[!tip] LA BEAM SEARCH ***NON* Ãˆ COMPLETA**

### Idea e pseudocodice
![Pasted image 20251025193001.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193001.jpg)

![Pasted image 20251025193007.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193007.jpg)


>[!tip]- Esempio
>![Pasted image 20251025193059.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193059.jpg)



>[!tip] Diverse applicazioni
>![Pasted image 20251025193237.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193237.jpg)
Tutti questi processi processi devono **scegliere una sequenza ottimale** di output fra milioni di possibilitÃ .  
â†’ Qui entra in gioco **Beam Search**, che esplora solo le migliori _K_ continuazioni invece di tutte.
>
>![Pasted image 20251025193250.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193250.jpg)
>Qui abbiamo un `beam = 2` (Ã¨ un'applicazione diretta dell'algoritmo)
>
>![Pasted image 20251025193302.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193302.jpg)
>Questa mostra **lâ€™applicazione pratica** del beam search durante lâ€™addestramento di un modello di _Speech Recognition_ o _Machine Translation_.


---

# IDA*
L'algoritmo IDA* combina
- A*
- ricerca in profonditÃ  iterativa (ID)
PiÃ¹ precisamente, combina i vantaggi di entrambe
- come A*: usa la funzione di valutazione $$f(n) = g(n) + h(n)$$
- come ID: esplora **in profonditÃ **, ma **con un limite**.

### Differenza chiave: il limite su `f`, non sulla profonditÃ 
Nel classico â€œapprofondimento iterativoâ€ (ID), il limite Ã¨ sulla **profonditÃ ** (quanti passi posso fare).

In IDA*, invece, il limite Ã¨ sul valore di **f(n)** (cioÃ¨ sul costo stimato totale).
Quindi:
- Si imposta un limite iniziale $f_{limit}$;
- Si esplora in profonditÃ  solo i nodi con $f(n) â‰¤ f_{limit}$
- Se non si trova la soluzione, si aumenta $f_limit$ e si ricomincia.

Questo processo si ripete finchÃ© non si trova una soluzione.

## Come funziona (riassunto operativo)
1. **Inizializza** $$f_{limit} = f(nodo \ iniziale)$$
2. **Ricerca in profonditÃ  limitata**  
    esplora tutti i nodi con$f(n) \le f_{limit}$
    
3. **Se non trovi il goal**, aumenta $f_{limit}$ al **minimo valore di f(n)** che ha superato il limite precedente
    
4. **Ripeti** finchÃ© non trovi il goal.

ðŸ‘‰ CosÃ¬ IDA* si espande â€œa stratiâ€ di f crescenti


## Ma di quanto deve essere aumentato il limite $f_{limit}$?
Abbiamo due casi principali
1. CASO 1 -- Costi fissi delle azioni
	- Se ogni passo costa la stessa quantitÃ  (es. 1), allora possiamo aumentare il limite di `1` a ogni azione
	
2. CASO 2 -- Costi variabili
	- Se i passi costano diversamente **non possiamo aumentare il limite di una quantitÃ  fissa**,   perchÃ© non sappiamo qual Ã¨ â€œil passo giustoâ€ per includere la prossima soluzione.
	ðŸ‘‰ Soluzione:  si guarda **tutti i nodi che sono stati scartati** perchÃ© avevano $f(n) \ge f_{limit}$
		Poi
		 - si prende **la piÃ¹ piccola di queste f**
		 - e la si usa come nuovo limite per la prossima iterazione


## Considerazioni
![Pasted image 20251025193501.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193501.jpg)

## Valutazioni funzioni euristiche
A paritÃ  di ammissibilitÃ , una euristica puÃ² essere piÃ¹ efficiente di un'altra nel trovare il cammino soluzione migliore (visitare meno nodi) IN BASE A QUANTO Ãˆ INFORMATA

>[!tip] PiÃ¹ informata = piÃ¹ vicina al costo reale $h^{*}(n)$

Abbiamo infatti
- $h(n) = 0$ -> minimo di informazione (BF o UC)
- $h^{*}(n)$ -> massimo di informazione (oracolo)
In generale, per le ***euristiche ammissibili*** $$0 \le h(n) \le h^{*}(n)$$


>[!lemma] TEOREMA
>Se $h_{1} \le h_{2}$, i nodi espansi da A* con $h_{2}$ sono **un sottoinsieme** dei nodi espansi da A* con $h_{1}$.
>Se $h_{1} \le h_{2}$, allora A* con $h_{2}$ Ã¨ **almeno efficiente quanto** A* con $h_{1}$

Questo perchÃ©
- se $h_{1} \le h_{2}$, vuol dire che $h_{2}$ **fornisce stime "piÃ¹ alte"** (vicine al valore reale ($h^{*}(n)$) pur restando **ammissibile** 
	- allora possiamo dire che $h_{2}$ Ã¨ **piÃ¹ precisa**.
		- piÃ¹ precisa vuol dire che **espande meno nodi**, da qui derivano
			- **SOTTOINSIEME** -> perchÃ© A* con $h_{2}$ visita meno nodi
			- **ALMENO EFFICIENTE** -> perchÃ© A* con $h_{2}$ impiega meno tempo (o al massimo lo stesso di A* con $h_{1}$)


>[!tip]- Esempio
>![Pasted image 20251025193619.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193619.jpg)
>La **distanza Manhattan** tra due caselle Ã¨ il **numero di mosse orizzontali e verticali** (non diagonali) necessarie per spostarsi da una posizione allâ€™altra.


## Compromesso tra costo del calcolo dell'euristica e il costo delle ricerca
![Pasted image 20251025193638.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193638.jpg)

Questo grafico mostra il compromesso tra quanto Ã¨ informata unâ€™euristica e il costo totale della ricerca.  
- Unâ€™euristica poco informata Ã¨ veloce da calcolare ma fa esplorare molti nodi, quindi la ricerca Ã¨ lenta.  
- Al contrario, unâ€™euristica molto precisa riduce la ricerca ma Ã¨ piÃ¹ costosa da calcolare.  
Il **costo complessivo** Ã¨ dato dalla somma di questi due effetti.  
Lâ€™obiettivo Ã¨ trovare un equilibrio: unâ€™euristica abbastanza informata da ridurre lo spazio di ricerca, ma non troppo complessa da rendere il calcolo inefficiente.


## Valutare l'efficacia di un'euristica
Per misurare quanto Ã¨ â€œforteâ€ o efficace unâ€™euristica possiamo utilizzare un valore chiamato **fattore di diramazione effettivo (`b*`)**.
>b* rappresenta **quanti nodi, in media, vengono generati per ogni nodo esplorato** durante la ricerca.

Per calcolarlo, si considerano:
- **N** = il numero totale di nodi generati dallâ€™algoritmo;
- **d** = la profonditÃ  della soluzione (cioÃ¨ quanti passi servono per arrivare al goal).

Poi si immagina un **albero uniforme** che ha lo stesso numero di nodi, e si risolve lâ€™equazione: $$N +1 = b^{*} + (b^{*})^{2} + ...+ (b^{*})^{d}$$per trovare $b^{*}$.

PiÃ¹ **b*** Ã¨ basso, **piÃ¹ lâ€™euristica Ã¨ efficace**, perchÃ© lâ€™algoritmo riesce a â€œstringereâ€ la ricerca esplorando meno nodi.  
In generale, una buona euristica ha **b*** vicino a 1 (di solito sotto 1.5).

>[!tip]- Esempi
>![Pasted image 20251025193652.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193652.jpg)

#### Esempio utile
![Pasted image 20251025193704.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193704.jpg)
![Pasted image 20251025193709.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193709.jpg)


---

# Come inventare un'euristica
Diverse strategie.

## Rilassamento del problema
Se il problema originale Ã¨ difficile da risolvere, possiamo â€œsemplificarloâ€ togliendo alcune regole o vincoli.  
> La soluzione del problema semplificato puÃ² darci una **stima del costo minimo** per risolvere quello reale â†’ cioÃ¨ una **euristica ammissibile**.

>[!tip]- Esempio
>
>![Pasted image 20251025193724.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193724.jpg)


## Massimizzazione di euristiche
Se abbiamo piÃ¹ euristiche **senza dominazione reciproca** (cioÃ¨ nessuna Ã¨ sempre â‰¥ dellâ€™altra),  
possiamo costruirne una nuova prendendo **il massimo** tra i loro valori: $$h(n) = max(h_{1}(n), h_{2}(n),...,h_{k}(n)$$
#### PerchÃ© funziona
- PoichÃ© **ogni háµ¢ Ã¨ ammissibile** (cioÃ¨ non supera mai il costo reale $h^*(n)$,  
    anche il loro massimo rimane **ammissibile**, perchÃ© non puÃ² superare $h^*(n)$.

- Inoltre, $h(n)$ sarÃ  **piÃ¹ informata** di ciascuna háµ¢ presa singolarmente,  
    perchÃ© in ogni nodo sceglie la stima piÃ¹ alta (cioÃ¨ la piÃ¹ precisa).



## Euristiche da sottoproblemi
Spesso un problema grande puÃ² essere diviso in **sottoproblemi piÃ¹ piccoli**, che sono piÃ¹ facili da risolvere.  

Lâ€™idea Ã¨:
> il costo per risolvere un sottoproblema Ã¨ sempre **una sottostima** del costo per risolvere lâ€™intero problema.

Il passo successivo Ã¨ rendere tutto automatico:
- si pre-calcola, una volta per tutte, **il costo minimo** per ogni possibile configurazione del sottoproblema;
    
- poi si salva tutto in un **database di pattern** (una tabella che mappa configurazioni â†’ costi).
    
- durante la ricerca, basta consultare il database per sapere subito il valore dellâ€™euristica $hDB(n)$.

#### Sottoproblemi multipli
Potremmo avere diversi sottoproblemi, ognuno che da origine a una **nuova euristica ammissibile**.
Possiamo combinarle in due modi:
1. PRENDERE IL MASSIMO $$h(n) = max(h_{1}(n), h_{2}(n),...)$$Questo funziona sempre e **resta ammissibile**, perchÃ© prendiamo la stima piÃ¹ alta senza mai superare il costo reale.
	
2. Sommare i valori
	In teoria, **sommare** piÃ¹ euristiche potrebbe dare una stima ancora piÃ¹ precisa, ma SOLO SE I SOTTOPROBLEMI SONO DISGIUNTI
		Questi casi si chiamano **pattern disgiunti**.  
			Ogni pattern (cioÃ¨ sottoproblema indipendente) ha il suo _pattern database_  
			con i costi pre-calcolati, e le loro somme danno una stima molto accurata.


## Apprendere dall'esperienza
Invece di scrivere a mano unâ€™euristica (come la distanza Manhattan, ecc.), possiamo **insegnare al programma** a stimare da solo il costo $h(n)$, basandosi sullâ€™esperienza acquisita durante lâ€™esecuzione.

Quindi lâ€™euristica non Ã¨ piÃ¹ definita da noi, ma **appresa automaticamente**.

![Pasted image 20251025193757.jpg](/img/user/ANNO%203/IA%20FOTO/Pasted%20image%2020251025193757.jpg)

## Combinazione di euristiche
A volte **una sola euristica non basta** per descrivere la difficoltÃ  di uno stato.  
In questi casi possiamo **combinare piÃ¹ euristiche diverse** in una **formula unica**, pesandole secondo la loro importanza: $$h(n) = c_{1}h_{1}(n) + c_{2}h_{2}(n) + ... + c_{k}h_{k}(n)$$Dove:
- $hiâ€‹(n)$ = una diversa funzione euristica (che misura un certo aspetto del problema)
- $c_{i}$ = un coefficiente che ne indica il peso o lâ€™importanza

>[!question] Come si scelgono i coefficienti `c`?
>Possono essere 
>- scelti a mano in base all'esperienza
>- oppure appresi automaticamente dal programma.


