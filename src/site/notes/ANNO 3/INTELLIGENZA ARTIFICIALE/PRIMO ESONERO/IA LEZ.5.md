---
{"dg-publish":true,"permalink":"/anno-3/intelligenza-artificiale/primo-esonero/ia-lez-5/"}
---


## Agenti classici
Gli agenti classici assumono che l'ambiente sia:
- **Completamente Osservabile:**¬†L'agente conosce tutto ci√≤ che √® rilevante per la sua decisione in ogni momento. Non ci sono informazioni nascoste.
- **Deterministico:**¬†Ogni azione ha un unico risultato, certo e prevedibile. Se un robot decide di andare avanti, andr√† avanti, senza possibilit√† di scivolare o deviare.
**La conseguenza di queste due assunzioni √® potentissima:**
- **Pianificazione Offline:**¬†L'agente pu√≤ calcolare l'intera sequenza di azioni per raggiungere l'obiettivo prima ancora di muovere un solo passo.
- **Esecuzione senza sorprese:**¬†Una volta creato, il piano pu√≤ essere eseguito a occhi chiusi, perch√© si √® certi che il mondo non cambier√† in modi imprevisti.

### Oggi vedremo algoritmi pi√π improntati al mondo reale

Nella realt√†, i problemi sono spesso pi√π complicati. La ricerca sistematica che esplora l'intero spazio degli stati pu√≤ diventare¬†**troppo costosa**¬†in termini di tempo e memoria, specialmente per problemi molto grandi. Inoltre, il mondo reale √® raramente cos√¨ semplice e prevedibile. Questo ci spinge a riconsiderare le nostre assunzioni e a cercare approcci pi√π flessibili e realistici.
#### üí° Introduzione alla Ricerca Locale

La prima grande evoluzione che studiamo √® la¬†**Ricerca Locale**¬†(Local Search). L'idea alla base √® un cambio di prospettiva radicale rispetto alla ricerca classica.
##### Il Percorso non Conta, Conta la Meta
Mentre gli algoritmi classici cercano un¬†**cammino soluzione**¬†(una sequenza di azioni per arrivare al goal), la ricerca locale si concentra su problemi dove¬†**la soluzione √® lo stato goal stesso**.
> In molti problemi, specialmente quelli di¬†**ottimizzazione**, non ci interessa¬†come¬†siamo arrivati a una soluzione, ma solo trovare lo¬†**stato finale**¬†che rappresenta la soluzione migliore.

Un esempio perfetto √® il problema delle 8 regine: non ci interessa la sequenza di mosse per posizionare le regine, ma solo la configurazione finale in cui nessuna regina minaccia le altre. In questo caso, partiamo da uno stato completo (tutte le regine sono sulla scacchiera, anche se in conflitto) e cerchiamo di migliorarlo passo dopo passo.
##### ‚öôÔ∏è Come Funziona la Ricerca Locale?
Le sue caratteristiche principali sono:
- **Non √® sistematica**: 
	- Non garantisce di esplorare tutte le possibilit√†.
- **Mantiene solo lo stato corrente**: 
	- A differenza degli algoritmi classici che memorizzano un'intera frontiera di nodi, la ricerca locale tiene traccia solo della posizione attuale (il¬†**nodo corrente**).
- **Si muove tra nodi adiacenti**: 
	- Ad ogni passo, valuta gli stati vicini e si sposta in uno di essi, sperando di migliorare la situazione.
- **Memoria super efficiente**: 
	- Non tenendo traccia dei cammini passati, consuma una quantit√† di memoria minima e costante.
- **Ideale per problemi di ottimizzazione**: 
	- √à perfetta per trovare lo stato "migliore" secondo una¬†**funzione obiettivo**¬†(che vogliamo massimizzare) o lo stato a¬†**costo minore**¬†(che vogliamo minimizzare).
### Lo spazio degli stati
![Pasted image 20251028154347.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251028154347.png)
L'algoritmo non vede tutta la mappa in una volta. Agisce come un esploratore che pu√≤ solo sondare il terreno immediatamente circostante.
- **La Superficie (il "terreno")**: Rappresenta l'intero¬†**spazio degli stati**. Ogni singolo punto sulla superficie √® una possibile configurazione del problema.
- **L'Altitudine (l'asse verticale)**: √à il valore della¬†**funzione obiettivo**¬†(o di valutazione). Misura la "bont√†" di uno stato. Pi√π alto √® il punto, migliore √® la soluzione (in un problema di massimizzazione).

1. **Stato Corrente**: L'algoritmo si trova in un punto (current state).
2. **Valutazione dei Vicini**: Esamina gli stati adiacenti (raggiungibili con una piccola mossa).
3. **Movimento**: Si sposta verso il vicino che ha l'altitudine migliore (pi√π alta o pi√π bassa, a seconda dell'obiettivo).


L'obiettivo finale √® trovare il punto migliore dell'intera mappa.
- **Ottimo Globale (global maximum)**: √à il picco pi√π alto in assoluto, ovvero la¬†**migliore soluzione possibile**.
Tuttavia, il paesaggio √® complesso e pieno di "trappole" che possono ingannare un algoritmo semplice.
- **Ottimo Locale (local maximum) ‚õ∞Ô∏è**:
    - **Cos'√®**: Un picco che √® pi√π alto di tutti i suoi vicini, ma¬†**non √® il picco pi√π alto**¬†dell'intera mappa.
    - **Il Problema**: Un algoritmo "ingordo" (greedy) che cerca solo il miglioramento immediato si¬†**bloccher√†**¬†qui, pensando di aver trovato la soluzione, perch√© ogni mossa lo porterebbe pi√π in basso.
- **Altopiano (shoulder¬†o¬†plateau) üèúÔ∏è**:
    - **Cos'√®**: Una zona piatta dove tutti i vicini hanno la stessa altezza.
    - **Il Problema**: L'algoritmo non sa in che direzione muoversi per migliorare e potrebbe vagare a caso o fermarsi.
### üßó‚Äç‚ôÇÔ∏è Ricerca in Salita (Hill Climbing): L'Approccio dell'Alpinista "Ingordo"

L'Hill Climbing √® l'algoritmo di ricerca locale pi√π semplice e intuitivo. Se immaginiamo il nostro problema come un paesaggio montuoso (come nella slide precedente), l'Hill Climbing si comporta come un alpinista che, ad ogni passo, sceglie la direzione che lo porta pi√π in alto possibile, senza avere una mappa completa.
√à un approccio¬†**greedy**¬†(ingordo), perch√© prende sempre la decisione che sembra migliore nell'immediato, senza alcuna pianificazione a lungo termine.
##### ‚öôÔ∏è Come Funziona il Processo?
L'algoritmo √® un ciclo continuo che si ripete fino a quando non pu√≤ pi√π migliorare:
1. **Partenza**: Si inizia da uno stato iniziale casuale (nodo-corrente).
2. **Esplorazione**: Si generano tutti gli stati successori (i "vicini") dello stato attuale.
3. **Valutazione**: Si calcola il valore della funzione obiettivo per ogni vicino.
4. **Movimento**: Ci si sposta nello stato che rappresenta un miglioramento.
5. **Ripetizione**: Il nuovo stato diventa il¬†nodo-corrente¬†e il ciclo ricomincia.

**Importante**: l'algoritmo non tiene traccia degli stati precedenti o di altre alternative. Mantiene in memoria solo la sua posizione attuale, rendendolo estremamente efficiente in termini di spazio.
#### üß≠ Le Varianti dell'Hill Climbing

Esistono diversi modi per scegliere il "passo" successivo. La prima slide ne elenca tre:
- **‚õ∞Ô∏è Salita Rapida (Steepest-Ascent)**: Si sceglie il vicino¬†**migliore in assoluto**, quello che garantisce il maggior guadagno. √à la versione descritta nello pseudocodice.
- **üé≤ Stocastico (Stochastic)**: Si sceglie¬†**a caso**¬†tra tutti i vicini che rappresentano un miglioramento. Non garantisce il passo migliore, ma √® pi√π veloce se valutare tutti i vicini √® costoso.
- **üëÜ Prima Scelta (First-Choice)**: Si generano i vicini uno alla volta e ci si sposta sul¬†**primo**¬†che risulta essere migliore dello stato attuale, senza esaminare gli altri. √à molto efficiente se ci sono tanti successori.
#### üîç Analisi dell'Algoritmo (Pseudocodice)
![Pasted image 20251028160108.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251028160108.png)
#### üõë Il Punto Debole: Il Fallimento
Come menzionato nella prima slide: "Se non ci sono stati successori migliori l'algoritmo termina con fallimento".

> **Cosa significa "fallimento"?**¬†Non significa che il programma va in crash. Significa che l'algoritmo¬†**fallisce nel trovare l'ottimo globale**. Si blocca in una soluzione sub-ottimale (un massimo locale) perch√© la sua natura "ingorda" e a breve termine gli impedisce di fare una mossa temporaneamente peggiore per raggiungere, in seguito, un picco pi√π alto.

### üëë Il Problema delle 8 Regine: Hill Climbing in Azione

Il problema delle 8 Regine √® l'esempio perfetto per vedere l'Hill Climbing al lavoro, con i suoi pregi e i suoi difetti. L'obiettivo √® posizionare 8 regine su una scacchiera in modo che nessuna possa attaccarne un'altra.

![Pasted image 20251028161005.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251028161005.png)

#### üéØ Preparare il Terreno: La Formulazione per la Ricerca Locale

Per usare l'Hill Climbing, dobbiamo definire tre cose fondamentali:

1. **Lo Stato**: Si parte da una configurazione "completa", cio√® con tutte le 8 regine gi√† sulla scacchiera, una per ogni colonna. Questo assicura che non ci siano mai conflitti verticali.
    
2. **La Funzione Obiettivo (h)**: √à il nostro "termometro" della qualit√†. In questo caso,¬†**h¬†= numero di coppie di regine che si attaccano a vicenda**¬†(orizzontalmente o in diagonale).
    
    - L'obiettivo finale √® trovare uno stato con¬†**h = 0**. Vogliamo quindi¬†**minimizzare**¬†il valore di¬†h.
        
3. **I Successori (le mosse)**: Uno stato successore si ottiene prendendo una regina e spostandola in una qualsiasi delle altre 7 caselle della sua stessa colonna.
    

> **Perch√© i successori sono 7x8?**  
> Semplice: abbiamo¬†**8**¬†regine (una per colonna) e per ognuna di esse ci sono¬†**7**¬†possibili nuove posizioni nella sua colonna. Totale mosse possibili: 8 √ó 7 = 56.

---

#### ‚öôÔ∏è L'Algoritmo al Lavoro: Un Passo di "Discesa"

La seconda slide ci mostra un passo concreto dell'algoritmo (nella sua versione a "salita rapida", che in questo caso √® una "discesa rapida" perch√© minimizziamo).

1. **Stato Iniziale**: Partiamo da una configurazione molto scarsa, con¬†h = 17¬†(17 coppie di regine in conflitto).
    
2. **Valutazione dei Successori**: L'algoritmo calcola il valore di¬†h¬†per ognuno dei 56 stati successori. La scacchiera con i numeri mostra esattamente questo: ogni numero in una casella vuota indica quale sarebbe il valore di¬†h¬†se spostassimo la regina di quella colonna in quella casella.
    
3. **La Scelta "Greedy"**: L'algoritmo cerca la mossa che porta al miglioramento pi√π grande, ovvero allo stato con il valore di¬†h¬†**pi√π basso**. In questo caso, il valore minimo che si pu√≤ ottenere √®¬†**12**.
    
4. **Gestione dei Pareggi**: Ci sono pi√π mosse che portano a uno stato con¬†h = 12. L'algoritmo ne¬†**sceglie una a caso**¬†e si sposta in quella nuova configurazione. Il ciclo poi ricomincia da l√¨.
    

---

#### üõë La Trappola: Bloccati in un Minimo Locale

La terza slide mostra il vero punto debole dell'Hill Climbing.

- **La Situazione**: L'immagine mostra una configurazione quasi perfetta, con¬†**h = 1**. C'√® solo una coppia di regine che si minacciano (evidenziate dal cerchio e dalla linea rossa).
    
- **Il Problema**: Da questa posizione,¬†**qualsiasi mossa possibile peggiora la situazione**. Se proviamo a spostare una qualsiasi regina per risolvere quell'unico conflitto, finiremo inevitabilmente per crearne di nuovi, risultando in uno stato con¬†h > 1.
    
- **La Conseguenza**: L'algoritmo Hill Climbing, valutando tutti i suoi 56 vicini e vedendo che nessuno di essi ha un valore¬†h¬†inferiore a 1, si convince di essere arrivato alla soluzione migliore possibile.
    
    > L'algoritmo¬†**si blocca e termina**, restituendo questa soluzione imperfetta.
    
Questa configurazione √® un¬†**minimo locale**: √® migliore di tutti i suoi vicini, ma non √® la soluzione ottima (h = 0), che sarebbe il¬†**minimo globale**.

### üßó‚Äç‚ôÄÔ∏è I Problemi con l'Hill Climbing: Perch√© l'Alpinista si Perde

Abbiamo visto che l'Hill Climbing √® veloce ma spesso fallisce. La prima slide riassume in modo visuale le "trappole" del paesaggio dello spazio degli stati che ingannano il nostro algoritmo "ingordo".

1. **‚õ∞Ô∏è Massimi Locali (le "colline")**:
    
    - **Il problema**: L'algoritmo raggiunge un picco (collina) che non √® il pi√π alto in assoluto (montagna). Da l√¨, ogni mossa √® in discesa, quindi l'algoritmo si ferma, convinto di aver finito. √à la trappola pi√π comune.
![Pasted image 20251028162118.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251028162118.png)

        
2. **üèúÔ∏è Altipiani (Plateaux)**:
    
    - **Il problema**: L'algoritmo arriva in una zona piatta, dove tutte le mosse vicine hanno lo stesso valore. Non essendoci una "salita" chiara, non sa dove andare e potrebbe vagare a caso o terminare prematuramente.

![Pasted image 20251028162130.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251028162130.png)

2. **üî™ Crinali (o Creste)**:
    
    - **Il problema**: Una situazione pi√π rara e complessa. Immagina di essere sulla cresta di una montagna. Per continuare a salire lungo la cresta, potresti dover fare una serie di mosse coordinate (es. due passi in diagonale). Un algoritmo che esamina una sola mossa alla volta (es. solo Nord, Sud, Est, Ovest) vedr√† solo discese ripide su entrambi i lati e si bloccher√†, non riuscendo a vedere il percorso complesso che gli permetterebbe di salire.
![Pasted image 20251028162157.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251028162157.png)
### üí™ Estensioni e Miglioramenti: Come Rendere l'Alpinista pi√π Intelligente

Dato che l'Hill Climbing di base √® inaffidabile, sono state sviluppate diverse strategie per aiutarlo a superare queste trappole.

#### 1. Consentire Mosse Laterali ‚ÜîÔ∏è
- **L'idea**: Per sfuggire agli altipiani e alle "spalle" delle colline, permettiamo all'algoritmo di muoversi "lateralmente", cio√® di spostarsi verso un vicino con lo¬†**stesso valore**, sperando che da l√¨ si apra un nuovo percorso in salita.
	- **Come funziona**: Si imposta un limite al numero di mosse laterali consecutive per evitare loop infiniti su un altopiano.
	- **Efficacia**: Nel problema delle 8 regine, questa semplice modifica aumenta la percentuale di successo dal 14% al¬†**94%**! Il prezzo da pagare √® un numero maggiore di passi per trovare la soluzione.
#### 2. Hill Climbing Stocastico üé≤
- **L'idea**: Invece di scegliere sempre il passo "migliore" (salita rapida), si sceglie¬†**a caso**¬†tra tutte le mosse che portano a un miglioramento.
	- **Perch√© funziona**: Introduce un elemento di casualit√† che pu√≤ aiutare a esplorare percorsi diversi, a volte meno ovvi ma pi√π promettenti. Si pu√≤ anche pesare la probabilit√† di scelta in base alla "pendenza" (i miglioramenti maggiori sono pi√π probabili).
	- **Efficacia**: Generalmente converge pi√π lentamente, ma a volte pu√≤ trovare soluzioni migliori evitando percorsi che portano a massimi locali poco interessanti.
#### 3. Hill Climbing Casuale con Prima Scelta üëÜ
- **L'idea**: Combina la casualit√† con l'efficienza. Invece di valutare tutti i vicini, li genera in ordine casuale e si sposta sul¬†**primo**¬†che trova che sia migliore dello stato attuale.
	- **Efficacia**: √à molto utile quando il numero di successori √® enorme. Invece di perdere tempo a calcolare il valore di tutti, si "accontenta" del primo miglioramento che trova, accelerando il processo.
#### 4. Hill climbing con riavvio casuale
- Se si blocca, RIPARTE DA UN NUOVO STATO INIZIALE CASUALE
	- Ripete questo processo pi√π volte finch√© non trova una soluzione ottimale.
	- TENDENZIALMENTE √® **completo** (basta ripetere!!)
	- Il funzionamento **non dipende molto dalla forma** del paesaggio, perch√© il riavvio consente di "saltare" in zone nuove.

## Simulated Annealing (Tempra simulata)
√à una combinazione tra:
- **Hill Climbing**, che cerca sempre di migliorare lo stato,
- e una **scelta stocastica controllata**, che _a volte_ accetta anche stati peggiori.

Questo serve a **scappare dai massimi locali**:  
accettare temporaneamente un peggioramento pu√≤ permettere di uscire da una ‚Äúcollina‚Äù e raggiungere un picco pi√π alto (la soluzione globale).

#### Il ruolo della temperatura
La temperatura `T` controlla quanto l‚Äôalgoritmo √® disposto ad accettare peggioramenti:
- all‚Äôinizio `T` √® alta ‚Üí le mosse peggiori vengono accettate spesso (esplorazione ampia);
- col tempo `T` scende ‚Üí il comportamento diventa pi√π ‚Äúrigido‚Äù, simile all‚ÄôHill Climbing.

üëâ Quando la temperatura √® molto bassa, l‚Äôalgoritmo si comporta come un normale Hill Climbing deterministico.


#### Funzionamento effettivo
A ogni iterazione, l‚Äôalgoritmo sceglie **un successore a caso** dello stato corrente (cio√® una nuova possibile soluzione vicina).  
Poi decide **se accettarlo o no**, secondo queste regole:
1. ***CASO 1***: il successore √® migliore della soluzione attuale
	- Se il nuovo stato `n'` ha un valore migliore di quello attuale `n` -> viene accettato subito (hill climbing classico)
	
2. ***CASO 2***: il successore √® peggiore
	- se `n'` √® peggiore (cio√® $\Delta E = f(n^{'}) - f(n) < 0$) -> l'algoritmo pu√≤ **accettarlo comunque**, ma con una **certa probabilit√†** $$p = e^{\Delta E/T}$$dove
		- $\Delta E$ √® il peggioramento 
		- $T$ √® la tempra corrente
		Poich√© $\Delta E < 0$, il valore di $e^{\Delta E/T}$ sar√† compreso tra `0` e `1`
	
	üëâ Quindi, l‚Äôalgoritmo **genera un numero casuale tra 0 e 1** e **accetta il nuovo stato** solo se il numero casuale √® **minore di p**.  
	In pratica: pi√π la mossa √® ‚Äúpoco peggiore‚Äù, pi√π √® probabile che venga accettata.

##### Ruolo della temperatura `T`
La temperatura **controlla la probabilit√† di accettare peggioramenti**:
- Quando `T` √® **alta**, anche stati peggiori vengono accettati spesso ‚Üí grande esplorazione.
- Quando `T` **scende**, la probabilit√† diminuisce ‚Üí il comportamento diventa pi√π ‚Äúrigido‚Äù, simile a Hill Climbing.

Man mano che l‚Äôalgoritmo avanza, `T` **decresce gradualmente** secondo un piano definito (chiamato _cooling schedule_), ad esempio: $$T_{k+1} = \alpha \cdot T_{k} \ \ \  \ \ \text{con 0} < \alpha \ \text{< 1}$$
#### Altre caratteristiche
- p √® **inversamente proporzionale al peggioramento**: se una mossa peggiora molto, sar√† accettata con una probabilit√† pi√π bassa.
- Col passare del tempo, `T` si riduce ‚Üí anche `p` si riduce ‚Üí l‚Äôalgoritmo diventa sempre pi√π selettivo.
- Alla fine, quando `T` √® molto bassa, il comportamento converge a un **Hill Climbing deterministico**.

### Algoritmo
![Pasted image 20251029193033.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251029193033.png)![Pasted image 20251029193044.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251029193044.png)

# Ricerca local beam
L‚Äôidea della **ricerca local beam** nasce per affrontare i limiti di memoria della ricerca locale tradizionale, che di solito mantiene un solo stato alla volta (come nell‚ÄôHill Climbing).  
In questo caso, invece, **vengono mantenuti contemporaneamente `k` stati**.

### Funzionamento
1. L‚Äôalgoritmo inizia con `k` stati generati casualmente.
2. A ogni passo:
    - Si generano **tutti i successori** di questi `k` stati.
    - Se uno dei successori √® una **soluzione**, l‚Äôalgoritmo termina.
    - Altrimenti, si **selezionano i `k` migliori successori** (in base alla funzione di valutazione) e si ripete il processo.

### Differenza con la ricerca a riavvio casuale
A prima vista, la local beam pu√≤ sembrare semplicemente **una versione parallela** della ricerca con riavvio casuale (che esegue pi√π ricerche indipendenti).  

In realt√†, **non √® cos√¨**:
- Nella **ricerca a riavvio casuale**, ogni ricerca procede indipendentemente.
- Nella **local beam**, invece, **le ricerche comunicano tra loro**:
    - le informazioni sui migliori stati trovati vengono condivise,
    - e le risorse si concentrano dove i progressi sono pi√π promettenti.

In questo modo, l‚Äôalgoritmo **abbandona rapidamente** le direzioni poco fruttuose e continua a esplorare quelle pi√π promettenti.

### ‚ö†Ô∏è Problema principale
Con il tempo, i `k` stati possono **convergere tutti nella stessa zona** dello spazio delle soluzioni (formando un _cluster_).  
Questo riduce la diversit√† della ricerca e la rende **simile a un Hill Climbing moltiplicato per `k`** ‚Äî quindi pi√π lenta ma non necessariamente pi√π efficace.

### üé≤ Variante: Ricerca Beam Stocastica
Per evitare questa mancanza di diversificazione, esiste una variante chiamata **ricerca beam stocastica**:
- invece di scegliere sempre i migliori `k` successori,
- si scelgono **in modo probabilistico**, con una probabilit√† **proporzionale al loro valore euristico**.

Cos√¨ si bilancia **esplorazione e sfruttamento**, evitando che tutti i cammini si concentrino sugli stessi stati promettenti.


---

# Ricerca con azioni non deterministiche
### üß© Contesto di partenza
Negli algoritmi di ricerca classici, si assume che:
- l‚Äôambiente sia **completamente osservabile** (cio√® l‚Äôagente sa sempre dove si trova);
- e che sia **deterministico**, quindi ogni azione ha **un solo risultato prevedibile**.

In questo caso, il piano √® semplicemente una **sequenza di azioni** fisse, decise in anticipo (_offline_), e le percezioni servono solo **all‚Äôinizio** per determinare lo stato iniziale.

### ‚öôÔ∏è Quando l‚Äôambiente non √® deterministico
In un ambiente **non deterministico** o **parzialmente osservabile**, la situazione cambia:
- l‚Äôagente **non conosce esattamente lo stato attuale** in cui si trova;
- e non pu√≤ sapere **in quale stato arriver√†** dopo aver eseguito un‚Äôazione.

Per gestire questa incertezza, l‚Äôagente mantiene un **insieme di stati possibili**, detto **stato-credenza** (_belief state_), cio√® l‚Äôinsieme di tutte le situazioni che ritiene plausibili.


### üß† Piani condizionali
In questi ambienti, una semplice sequenza di azioni non basta.  
Serve un **piano condizionale** (o _piano di contingenza_), cio√® una **strategia** che specifica:
> ‚Äúcosa fare in base a ci√≤ che l‚Äôagente percepisce lungo il percorso‚Äù.

In pratica, il piano si adatta **alle percezioni e agli eventi imprevisti** durante l‚Äôesecuzione.


>[!tip]- üßπ Esempio: Il mondo dell‚Äôaspirapolvere erratico
In questo ambiente:
>
>- Se l‚Äôazione **Aspira** √® eseguita su un riquadro **sporco**, pu√≤:
>    
>    - pulirlo normalmente,
>        
>    - **e a volte pulire anche un riquadro adiacente**.
>        
>- Se invece √® eseguita su un riquadro **gi√† pulito**, pu√≤ **sporcarlo di nuovo** in modo casuale.
>    
>
>Quindi l‚Äôambiente √® **non deterministico**, perch√© lo stesso comando pu√≤ produrre **risultati diversi**.


### üîÑ Generalizzazione del modello di transizione
Per modellare questo comportamento, si passa:
- da una funzione classica **Risultato(s, a)** ‚Üí _ritorna un solo stato_,
- a una nuova funzione **Risultati(s, a)** ‚Üí _ritorna un insieme di stati possibili_.

In questo modo possiamo rappresentare tutti gli esiti che un‚Äôazione pu√≤ produrre.


---

# Alberi di ricerca AND-OR
Quando ci troviamo in **ambienti non deterministici**, l‚Äôalbero di ricerca classico (quello usato negli ambienti deterministici) **non basta pi√π**.  
In questi casi si utilizza una struttura chiamata **albero AND‚ÄìOR**, che rappresenta sia **le scelte dell‚Äôagente** sia **le incertezze dell‚Äôambiente**.
### üß†Differenza tra nodi OR e nodi AND
- **Nodi OR** ‚Üí rappresentano **le scelte dell‚Äôagente**  
    Esempio: ‚ÄúPosso muovermi a destra oppure aspirare‚Äù.  
    L‚Äôagente sceglie **una sola** delle azioni disponibili.
    
- **Nodi AND** ‚Üí rappresentano **le diverse conseguenze possibili di una stessa azione**, causate dall‚Äôambiente non deterministico.  
    Esempio: ‚ÄúSe aspiro, potrei ottenere due risultati diversi: il pavimento resta sporco oppure si pulisce‚Äù.

Questi due tipi di nodi **si alternano** nell‚Äôalbero:
- ai nodi OR corrispondono le decisioni dell‚Äôagente,
- ai nodi AND corrispondono i possibili risultati dell‚Äôambiente.

Il risultato √® un **albero di ricerca AND‚ÄìOR**, dove la ricerca deve tener conto sia delle proprie scelte che delle reazioni dell‚Äôambiente.

![Pasted image 20251029193155.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251029193155.png)
- NODI OR -> cerchio singolo
- NODI AND -> cerchio singolo + semicerchio


### ‚úÖ Soluzione di un problema AND‚ÄìOR
Una **soluzione** non √® una semplice sequenza di azioni (come nei problemi deterministici), ma un **sottoalbero** dell‚Äôalbero di ricerca che rispetta tre condizioni:
1. Ogni **foglia** del sottoalbero √® un **nodo obiettivo**.
2. In ogni **nodo OR**, √® specificata **una sola azione** da eseguire.
3. In ogni **nodo AND**, **tutti i rami** devono essere inclusi, perch√© rappresentano i diversi possibili risultati dell‚Äôambiente.


### Pseudocodice
![Pasted image 20251029193206.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251029193206.png)


## Soluzioni cicliche per problemi non deterministici
Pensiamo al **mondo dell‚Äôaspirapolvere scivoloso**, una variante del classico problema dell‚Äôaspirapolvere in cui:
- le azioni di movimento (**Sinistra**, **Destra**) **a volte falliscono**,
- e quindi l‚Äôagente pu√≤ **rimanere fermo** anche se ha tentato di spostarsi.

### üß© Problema
A causa di questo comportamento **non deterministico**, non esiste una **soluzione aciclica** che porti sempre con certezza allo stato obiettivo.  
Se usassimo la normale ricerca AND‚ÄìOR, l‚Äôalgoritmo **fallirebbe**, perch√© non troverebbe un piano che garantisca il successo in tutti i casi.

### üîÅ Soluzione ciclica
Esiste per√≤ una **soluzione ciclica**, che consiste nel **ripetere un‚Äôazione finch√© non riesce**.  
Per esempio:
> ‚ÄúContinua a provare **Destra** finch√© non ti sposti davvero a destra‚Äù.

In pseudocodice, questa soluzione si esprimerebbe con un costrutto simile a:
```scss
while (non sei nel riquadro destro)
    esegui Destra
```

### Esempio visivo
![Pasted image 20251029193219.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251029193219.png)

### ‚öôÔ∏è Condizioni di correttezza
Perch√© un piano ciclico sia valido, devono valere due condizioni:
1. **Ogni foglia** (cio√® ogni possibile esito del piano) deve essere uno **stato obiettivo**;
2. Da **ogni punto del piano**, deve esserci **almeno un percorso** che porta a una foglia obiettivo.

In altre parole, qualunque cosa accada, esiste sempre una sequenza di azioni che **prima o poi** porta alla soluzione.

>[!danger] ATTENZIONE: l'ultima frase di prima √® vera SE E SOLO SE il fallimento √® dovuto a una casualit√†; se invece il fallimento √® dovuto a una condizione fissa e non osservata, ripetere non cambier√† nulla. 


---

# Ricerca con osservazioni parziali
Quando l‚Äôagente si trova in un ambiente **parzialmente osservabile**, le sue percezioni **non bastano per sapere con certezza in quale stato si trova**.  
In questi casi, l‚Äôagente deve **gestire l‚Äôincertezza**, e a volte alcune delle sue azioni servono **non tanto a raggiungere un obiettivo**, ma a **raccogliere informazioni** e ridurre il dubbio sul proprio stato attuale.

### üß† Ricerca in assenza di osservazioni (problema senza sensori)
Se l‚Äôagente **non riceve nessuna informazione** dal mondo (cio√® non ha sensori), si parla di **problema senza sensori** o **problema conformante**.

In questo tipo di problema:
- L‚Äôagente non sa dove si trova n√© lo stato esatto del mondo.
- Tuttavia, pu√≤ comunque **ragionare su un insieme di stati possibili**, chiamato **stato-credenza (belief state)**.


>[!tip]- üß© Esempio: il mondo dell‚Äôaspirapolvere senza sensori
Immaginiamo il mondo dell‚Äôaspirapolvere deterministico, ma:
>- l‚Äôagente **conosce la mappa** del suo ambiente (sa che ci sono due stanze, ad esempio),
>- per√≤ **non sa dove si trova n√© quali riquadri sono sporchi**.
>
>Il suo stato iniziale, quindi, non √® un singolo stato fisico, ma **l‚Äôinsieme di tutti gli stati possibili**:
>
> {1, 2, 3, 4, 5, 6, 7, 8}
>
>Questo insieme rappresenta il **suo stato-credenza iniziale**.


### ‚öôÔ∏è Come si svolge la ricerca
In questo caso, la ricerca non si svolge nello spazio degli stati reali, ma nello **spazio degli stati-credenza**.  
Ogni stato-credenza rappresenta **tutte le situazioni fisiche possibili** in cui l‚Äôagente potrebbe trovarsi.

In questo nuovo spazio:
- il problema diventa **completamente osservabile**,  
    perch√© l‚Äôagente conosce sempre **il proprio stato-credenza**, anche se non sa quale stato fisico specifico sta vivendo.


### üß© Componenti del problema di stati-credenza
1. **Stati:**  
    Ogni stato-credenza √® un sottoinsieme degli stati fisici originali.  
    Se il problema `P` ha **`N` stati fisici**, allora il nuovo problema pu√≤ avere fino a **2‚Åø stati-credenza**, anche se non tutti sono effettivamente raggiungibili.
    
2. **Stato iniziale:**  
    Di solito √® l‚Äôinsieme di **tutti gli stati fisici possibili**, ma pu√≤ essere ridotto se l‚Äôagente ha informazioni parziali.
    
3. **Azioni:**  
    Sono l‚Äôinsieme di tutte le azioni possibili nei vari stati fisici inclusi nello stato-credenza.  
    In pratica:
    - se un‚Äôazione √® lecita in almeno uno degli stati, pu√≤ essere considerata;
    - ma se √® pericolosa o dannosa in certi stati, conviene includere solo quelle **sicure in tutti**.
    
4. **Modello di transizione:**  
    L‚Äôeffetto di un‚Äôazione su uno stato-credenza √® l‚Äô**unione** di tutti gli stati ottenibili applicando quell‚Äôazione a ciascuno degli stati possibili del belief state.
    
5. **Test obiettivo:**  
    Uno stato-credenza soddisfa l‚Äôobiettivo se **almeno uno dei suoi stati fisici** soddisfa la condizione obiettivo.
    
6. **Costo dell‚Äôazione:**  
    Se un‚Äôazione ha **costi diversi** nei vari stati, il costo nel belief state pu√≤ essere:
    - il **valore medio** dei costi possibili,
    - oppure una **stima prudente** (es. il massimo), in base all‚Äôapproccio adottato.


### Come vengono aggiornati gli stati credenza (versione deterministica e non)
![Pasted image 20251029193235.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251029193235.png)

### Spazio degli stati completo
![Pasted image 20251029193247.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251029193247.png)
Quando si lavora con stati-credenza, lo spazio cresce esponenzialmente: nel caso dell‚Äôaspirapolvere con 8 stati fisici si avrebbero 2‚Å∏ = 256 stati-credenza, ma solo 12 realmente raggiungibili.  
Per evitare esplorazioni inutili si usa una **ricerca su grafo**, ignorando gli stati gi√† visitati.  

Si pu√≤ anche **potare** in modo pi√π efficiente:
- se uno stato gi√† incontrato `s'` √® contenuto in `s` (`s'‚äÇs`), `s` √® inutile e si scarta;
- se `s` √® contenuto in `s'` (`s‚äÇs'`) e da `s'` esiste una soluzione, anche `s` si pu√≤ scartare.

Poich√© rappresentare tutti gli stati √® costoso, si pu√≤ applicare una **ricerca incrementale**:  
si trova una soluzione per un singolo stato, poi si verifica se funziona anche per gli altri.  
Questo approccio riduce i fallimenti precoci e mira a ottenere **una sola soluzione valida per tutti gli stati possibili**.


---

# Ricerca in ambienti parzialmente osservabili
In molti casi, un agente **non pu√≤ risolvere un problema senza sensori**, perch√© non saprebbe mai in quale stato si trova.  

Per gestire questo tipo di situazioni, nella definizione del problema si introduce una funzione chiamata **Percezione(s)**, che restituisce la percezione che l‚Äôagente riceve quando si trova nello stato `s`.

Se i sensori sono **non deterministici**, la funzione diventa **Percezioni(s)**, e restituisce **un insieme di percezioni possibili**.
- Nei problemi **completamente osservabili**, vale **Percezione(s) = s**, perch√© l‚Äôagente conosce lo stato con certezza.
- Nei problemi **senza sensori**, invece, **Percezione(s) = null**, poich√© l‚Äôagente non riceve alcuna informazione.

### üß† Transizione in ambienti parzialmente osservabili
Quando l‚Äôagente esegue un‚Äôazione, il passaggio da uno stato-credenza al successivo avviene in **tre fasi distinte**:
1. **Fase di predizione**  
    Si calcola lo **stato-credenza previsto** dopo aver eseguito un‚Äôazione, esattamente come nel caso senza sensori.  
    
2. **Fase delle percezioni possibili**  
    A partire dallo stato-credenza previsto, si calcola l‚Äôinsieme di **tutte le percezioni** che l‚Äôagente potrebbe osservare.  
    Questa fase serve a stimare che tipo di informazioni i sensori potranno fornire.
    
3. **Fase di aggiornamento**  
    Per ogni possibile percezione, si calcola il nuovo **stato-credenza aggiornato**, che contiene solo gli stati coerenti con quella percezione.  
    In altre parole, si eliminano gli stati incompatibili con ci√≤ che l‚Äôagente ha percepito.

Mettendo insieme le tre fasi (predizione ‚Üí percezioni ‚Üí aggiornamento), si ottiene l‚Äôinsieme **dei possibili stati-credenza** che possono risultare da una data azione, tenendo conto anche delle percezioni future.

Durante la pianificazione, l‚Äôagente **non conosce ancora le percezioni future** che ricever√†, ma deve comunque tenerne conto.  


---

# Agenti per ricerca online e ambienti sconosciuti
Finora abbiamo parlato di agenti che usano **ricerca offline**, cio√® che **calcolano tutto il piano d‚Äôazione prima di iniziare ad agire**.  

Negli **ambienti reali**, per√≤, questo approccio non sempre √® possibile:  
l‚Äôambiente pu√≤ cambiare, le informazioni possono essere incomplete o il tempo per pianificare pu√≤ essere limitato.  
In questi casi entra in gioco la **ricerca online**.

### üß† Cos‚Äô√® la ricerca online
Nella **ricerca online**, l‚Äôagente **non pianifica tutto in anticipo**, ma alterna continuamente:
1. **Azione** ‚Äì esegue un passo nell‚Äôambiente;
2. **Osservazione** ‚Äì percepisce il nuovo stato o le conseguenze dell‚Äôazione;
3. **Aggiornamento** ‚Äì decide la prossima mossa in base a ci√≤ che ha imparato.

La ricerca online √® particolarmente efficace in:
- **ambienti dinamici o semidinamici**, dove lo stato del mondo cambia rapidamente e non c‚Äô√® tempo per calcolare tutto in anticipo;
- **ambienti non deterministici**, dove le azioni possono produrre risultati diversi, e l‚Äôagente deve reagire alle situazioni reali che si verificano, invece di pianificare per tutte le possibilit√† teoriche.


>[!tip] PIANIFICAZIONE vs AZIONE
>C‚Äô√® un compromesso importante: pi√π un agente pianifica in anticipo, meno rischia di trovarsi in difficolt√†; ma pi√π pianifica, pi√π tempo impiega prima di agire.
>
>Un buon agente deve quindi **bilanciare** il tempo speso a pianificare e quello speso ad agire.


### In ambienti sconosciuti
In un **ambiente sconosciuto**, l‚Äôagente **non conosce gli stati n√© gli effetti delle azioni**.  
Deve quindi imparare tutto **sperimentando**:
- ogni azione diventa un **test**,
- le osservazioni raccolte servono per **costruire progressivamente un modello** dell‚Äôambiente.

Un esempio tipico √® il **problema della costruzione di mappe**:  
un robot esplora un ambiente che non conosce, registrando passo dopo passo la posizione degli ostacoli e aggiornando la sua mappa interna.


### Problemi di ricerca online
Un **problema di ricerca online** si risolve attraverso tre attivit√† fondamentali: **elaborazione, percezione e azione**.  
A differenza della ricerca offline, l‚Äôagente **non pu√≤ conoscere in anticipo** il risultato di un‚Äôazione:  
non pu√≤ cio√® determinare **Risultato(s, a)** se non **trovandosi effettivamente nello stato `s` ed eseguendo l‚Äôazione `a`**.

In alcuni casi, l‚Äôagente pu√≤ disporre di una **funzione euristica ammissibile h(s)**, che fornisce una stima della distanza tra lo stato corrente e uno stato obiettivo.

#### üß© Assunzioni per un problema di esplorazione
Nel contesto della ricerca online, si assumono le seguenti condizioni:
- Solo **lo stato corrente** √® osservabile, mentre l‚Äôambiente √® **ignoto**.
- Non si conoscono **gli effetti** delle azioni n√© il **loro costo**.
- Gli **stati futuri** e le **azioni possibili** non sono noti a priori.
- L‚Äôagente deve eseguire **azioni esplorative** come parte della risoluzione del problema.

#### üß† Conoscenze dell‚Äôagente online nello stato s
Quando l‚Äôagente si trova in uno stato `s`, le sue conoscenze sono limitate a:
- le **azioni legali** nello stato attuale;
- il risultato **Risultato(s, a)**, ma solo **dopo aver eseguito l‚Äôazione `a`**;
- il **costo della mossa** $c(s, a, s^{'})$, anch‚Äôesso noto solo dopo l‚Äôesecuzione;
- il **Goal-Test(s)**, per verificare se lo stato √® un obiettivo;
- la **stima della distanza** dal goal fornita dalla funzione euristica `h(s)`.

#### ‚öôÔ∏è Obiettivo e costo della ricerca
Generalmente, lo scopo dell‚Äôagente √® **raggiungere uno stato obiettivo minimizzando il costo complessivo del percorso**.  
Questo costo rappresenta **la somma effettiva dei costi delle azioni realmente eseguite**.

√à prassi comune confrontare questo costo con quello che l‚Äôagente **avrebbe sostenuto conoscendo gi√† l‚Äôintero spazio di ricerca** (cio√® il cammino ottimo in un ambiente noto).  
Il rapporto tra questi due valori √® chiamato **rapporto di competitivit√† (competitive ratio)**, e idealmente dovrebbe essere **il pi√π piccolo possibile**.

#### ‚ö†Ô∏è Limiti e problemi della ricerca online
Gli agenti di ricerca online sono **vulnerabili ai vicoli ciechi**, ossia a stati dai quali **non √® pi√π possibile raggiungere l‚Äôobiettivo**.  
Se l‚Äôagente **non conosce il significato delle azioni**, pu√≤ compiere scelte che lo portano in situazioni **irreversibili** o da cui non pu√≤ pi√π uscire.

In generale, **nessun algoritmo pu√≤ evitare i vicoli ciechi in tutti gli spazi degli stati**.  
Gli ambienti sono **esplorabili in modo sicuro** solo se:
- **non esistono azioni irreversibili**, e
- **lo stato obiettivo √® sempre raggiungibile**.

Tuttavia, anche in questi casi, **non √® garantito un rapporto di competitivit√† limitato**, quindi la ricerca online pu√≤ risultare comunque meno efficiente rispetto a una pianificazione offline completa.


## Agenti per ricerca online
Gli agenti online ad ogni passo decidono l'**azione da fare** (non il piano) e la eseguono.
La ricerca in profondit√† online consiste nell‚Äôesplorazione sistematica delle alternative, √®
necessario ricordarsi ci√≤ che si √® scoperto. 
Il backtracking significa appunto tornare sui propri passi.


### Ricerca locale online
Nella **ricerca online**, il valore della **funzione euristica** √® conosciuto **solo dopo aver esplorato effettivamente uno stato**.  
Come la ricerca in profondit√†, anche la **ricerca Hill Climbing** √® locale: infatti espande solo gli stati vicini e **mantiene in memoria un solo stato per volta**.  
Per questo motivo, l‚ÄôHill Climbing pu√≤ essere considerato **un algoritmo gi√† online**.


### ‚ö†Ô∏è Limiti dell‚ÄôHill Climbing
Nonostante la sua natura locale, l‚ÄôHill Climbing **non √® efficace per l‚Äôesplorazione**, perch√©:
- pu√≤ **bloccarsi in un massimo locale**;
- non pu√≤ utilizzare **riavvii casuali** come nella versione offline, poich√© l‚Äôagente non ha la possibilit√† di ‚Äúteletrasportarsi‚Äù in un nuovo stato iniziale.


### üß© Alternative ai riavvii casuali
Per superare questi limiti, si possono usare due varianti:
1. **Random Walk**  
    L‚Äôagente, in alcuni casi, sceglie **casualmente una delle azioni possibili** nello stato corrente.  
    Questo introduce una componente di casualit√† che pu√≤ aiutarlo a uscire da massimi locali.
    
2. __Apprendimento Real-Time (LRTA)__  
    In alternativa, si pu√≤ rendere l‚ÄôHill Climbing **pi√π intelligente** aggiungendo **memoria e apprendimento** anzich√© casualit√†.  
    L‚Äôagente **aggiorna i valori euristici** man mano che esplora, rendendoli progressivamente pi√π realistici.  
    Questo approccio √® chiamato **LRTA*** (_Learning Real-Time A*_).


### ‚öôÔ∏è Funzionamento di LRTA
L‚Äôalgoritmo LRTA* simula il comportamento di A*, ma **in tempo reale** e **in modo locale**:
- aggiorna la **stima del costo** dello stato appena lasciato;
- poi sceglie la **mossa apparentemente migliore** in base alle stime correnti della funzione euristica `H`.  
    In questo modo l‚Äôagente impara progressivamente a valutare meglio i costi reali del percorso.
![Pasted image 20251030112816.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251030112816.png)
![Pasted image 20251030112825.png](/img/user/ANNO%203/FOTOANNO3/IA%20FOTO/Pasted%20image%2020251030112825.png)


### üìä Propriet√† di LRTA
- √à **completo** negli spazi **esplorabili in modo sicuro** (cio√® senza azioni irreversibili).
- Nel **caso peggiore**, visita **ogni stato due volte**, ma in media √® **pi√π efficiente della ricerca in profondit√† online**.
- **Non √® ottimale**, a meno che l‚Äôagente non disponga di **un‚Äôeuristica perfetta**.
