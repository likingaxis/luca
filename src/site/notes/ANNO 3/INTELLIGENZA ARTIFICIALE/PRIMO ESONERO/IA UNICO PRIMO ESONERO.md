---
{"dg-publish":true,"permalink":"/anno-3/intelligenza-artificiale/primo-esonero/ia-unico-primo-esonero/"}
---

## Definizione di IA
L‚Äô**Intelligenza Artificiale (IA)** √® la disciplina che studia **come creare sistemi capaci di osservare, comprendere e riprodurre comportamenti intelligenti**.  
In altre parole, mira a costruire macchine in grado di **percepire l‚Äôambiente**, **ragionare sulle informazioni ricevute** e **agire in modo autonomo** per raggiungere un obiettivo.
## Definizione di Agente
Nel linguaggio dell‚ÄôIA, un **agente** √® un‚Äôentit√† autonoma capace di **percepire l‚Äôambiente** (attraverso percettori) e **intervenire su di esso** attraverso le proprie azioni con gli attuatori.  
Un singolo sistema pu√≤ essere considerato un agente, ma in molti casi si parla di **sistemi multi-agente**, in cui pi√π entit√† collaborano o competono per raggiungere determinati obiettivi.
## IA FORTE VS DEBOLE
L‚ÄôIA pu√≤ essere studiata e progettata da diverse prospettive. Le due principali sono:
- **IA FORTE**
	- **L‚ÄôIA forte** si pone l‚Äôobiettivo ambizioso di *riprodurre il ragionamento umano*.
	- Non si limita quindi a fornire risposte corrette, ma cerca di *simulare i processi cognitivi* che portano un essere umano a elaborarle.
- **IA DEBOLE**
	- L‚Äô**IA debole**, al contrario, punta a *risolvere problemi pratici specifici*, senza preoccuparsi di replicare il pensiero umano.  
	- L‚Äôobiettivo √® ottenere *prestazioni efficaci*, anche se il sistema non ‚Äúcomprende‚Äù davvero ci√≤ che fa.  

## UMANIT√Ä O RAZIONALIT√Ä
![Pasted image 20251013184910.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251013184910.png)
L‚ÄôIA pu√≤ essere progettata per ispirarsi a due diversi principi:

- il **comportamento umano**, che riflette come le persone pensano e agiscono nella realt√†;
- la **razionalit√† pura**, cio√® l‚Äôottimizzazione logica delle decisioni.
## Problem Solving
Una delle funzioni centrali dell‚ÄôIA √® il **problem solving**, ossia la capacit√† di risolvere problemi complessi partendo da informazioni incomplete o ambigue.

## Instruction Tuning
L‚Äô**Instruction Tuning** √® una tecnica di addestramento in cui il modello impara a **seguire istruzioni testuali** in modo coerente.  
L‚Äôinput √® spesso costituito da una *premessa* seguita da una *domanda*, che attiva un processo di ragionamento iterativo.

## Prompt
Il **prompt** rappresenta la **base di comunicazione tra l‚Äôutente e il modello**.  
√à una combinazione di tre elementi:
- *premessa*, contesto o introduzione al compito da eseguire
- *domanda*, ci√≤ che si chiede al modello
- *contesto*, informazioni aggiuntive

## Il Test di Turing

Nel 1950, **Alan Turing** propose un esperimento per stabilire se una macchina potesse essere considerata intelligente.

> ‚ÄúUn sistema √® intelligente se un osservatore umano, dialogando con esso, non riesce a distinguere se sta parlando con una persona o con una macchina.‚Äù


LEZ 2 -> 

---  
## L‚ÄôEffetto IA
Ogni volta che una tecnologia basata sull‚ÄôIA diventa di uso comune, **smettiamo di percepirla come intelligenza artificiale**.  
√à il fenomeno noto come **Effetto IA**.

## Ciclo di un agente intelligente
- Ciclo dell'agente
	- *percepire una azione mediante i sensori*
		- riceve dei dati
	- *decido*
		- elabora la percezione ed effettua la sua funzione agente
	- *agisco con una azione*
		- esegue l'azione scelta anche mediante gli effettori
	- *si aggiorna*
		- aggiorna l'ambiente contando l'azione appena eseguita
![Pasted image 20251013185628.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251013185628.png)
## Percezioni e Azioni dell'agente
- come abbiamo detto l'agente riceve *percezioni* ,ovvero l'**input ricevuto dai sensori** dell'agente
	- l'insieme di tutte le percezioni passate rappresentano la *Sequenza Percettiva*
- La *Funzione Agente* serve per definire le azioni che esso pu√≤ eseguire
	- Definisce l'**azione da compiere per ogni possibile sequenza percettiva**
	    - √à una **descrizione matematica astratta** del comportamento dell'agente: $f: P^* \to A$ 
		    - (dove $P^*$ √® l'insieme delle sequenze percettive e $A$ √® l'insieme delle azioni)
- Il *programma Agente* √® l'**implementazione concreta** della funzione agente, in esecuzione all'interno di un sistema fisico (l'architettura dell'agente)
## Struttura di un Agente con Ambiente
![Pasted image 20251016085815.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016085815.png)

## Agenti Razionali
- quando si parla di *agente intelligente* si intende proprio *agente razionale*
	- **Razionalit√† $\neq$ Onniscienza:** Un agente razionale non √® necessariamente **onnisciente** o addirittura *onnipotente*
- L'agente Razionale √® colui che per ogni possibile sequenza di percezioni, cerca di scegliere un'azione che **massimizzi il valore atteso della sua misura di prestazione**, considerando le sue percezioni passate e le sue capacit√†. 
	- Per **Misura di Prestazione** si intende un criterio volto a valutare l'efficacia di una azione scelta dall'agente 
		- Per costruire una misura di prestazione utile, bisogna considerare **due aspetti fondamentali**:
			- *Natura* della misura esterna
				- La misura deve essere **esterna all‚Äôagente**, cio√® guardare **agli effetti che le sue azioni producono sull‚Äôambiente**, non ai processi interni.
			- *Scopo* della misura ‚Üí ‚ÄúCriterio del progettista‚Äù
				- La misura della prestazione √® quindi uno strumento per **definire la razionalit√†** del comportamento: ossia, cosa significa ‚Äúagire bene‚Äù in quel contesto.
				- questo √® demandato al progettista

## 4 fattori della razionalit√†
La scelta razionale di un agente in un dato momento dipende da quattro elementi chiave:
- *Misura di prestazione*
	- Un'azione √® razionale solo se contribuisce positivamente al raggiungimento degli obiettivi stabiliti da questa misura
- *Conoscenza pregressa*
	- La conoscenza pregressa (come ad esempio le regole della fisica o la mappa di un'area) aiuta l'agente a **prevedere le conseguenze** delle sue azioni
- *Percezioni presenti e passate*
	- Tutta la storia delle percezioni ricevute dai sensori.
- *Capacit√† dell'agente*
	- Un'azione √® razionale solo se rientra nelle **possibilit√† di esecuzione** dell'agente.
## Agenti autonomi
Un agente √® definito *autonomo* nella misura in cui il suo comportamento dipende dalla sua esperienza (cio√®, dalle percezioni passate e dall'apprendimento che ne deriva). 
## Definizione di Ambiente Operativo e PEAS
- Il **problema P** di un agente, ovvero la sua attivit√† principale, √® definita dalla **caratterizzazione adeguata dell'ambiente operativo**.
- La progettazione di un **agente razionale** corrisponde alla **soluzione** per questo problema.

Il framework **PEAS** fornisce un metodo sistematico per specificare *l'ambiente operativo*, identificando i quattro fattori cruciali che influenzano la progettazione dell'agente:
##### **P**erformance (Prestazioni)
- **Definizione:** La **misura di prestazione** che definisce il **criterio del successo**. Valuta la sequenza di stati dell'ambiente per determinare la desiderabilit√† del comportamento dell'agente.
- _Esempio: Autista di taxi automatico:_ Sicurezza, velocit√†, legalit√†, massimizzazione dei profitti.
##### **E**nvironment (Ambiente)
- **Definizione:** La **parte dell'universo** di cui ci interessa lo stato quando progettiamo l'agente ‚Äî la parte che influenza ci√≤ che l'agente percepisce e sulla quale influiscono le sue azioni.
- _Esempio: Autista di taxi automatico:_ Strade, altri veicoli nel traffico, pedoni, clienti, tempo atmosferico.
##### **A**ctuators (Attuatori)
- **Definizione:** I **mezzi** che l'agente utilizza per **agire sull'ambiente**.
- _Esempio: Autista di taxi automatico:_ Sterzo, acceleratore, freni, clacson.
##### **S**ensors (Sensori)
- **Definizione:** I **dispositivi** attraverso i quali l'agente **percepisce** il suo ambiente. Il dato percepito √® chiamato **percezione** (_percept_).
- _Esempio: Autista di taxi automatico:_ Telecamere, radar, tachimetro, GPS.
#### Esempio con Chat GPT
![Pasted image 20251016093738.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016093738.png)
## Propriet√† dell‚Äôambiente e del problema
I diversi ambienti di lavoro di un agente si caratterizzano lungo alcune dimensioni che ne influenzano la complessit√† e la progettazione.
### Osservabilit√†
- **Completamente Osservabile**
	- L'agente ha accesso allo **stato completo** dell'ambiente in ogni momento, attraverso i suoi sensori. √à sufficiente che i sensori misurino tutti gli aspetti _rilevanti_ per la scelta dell'azione.

- **Parzialmente Osservabile**
	- L'agente non pu√≤ vedere l'intero stato dell'ambiente a causa di sensori rumorosi, inaccurati o incompleti. Se l'agente non ha sensori, l'ambiente √® **inosservabile**.
### Numero di agenti
- **Agente Singolo**
	- L'ambiente contiene solo un agente che opera e le "altre" entit√† possono essere trattate come semplici oggetti che si comportano secondo le leggi della fisica.

- **Multi-Agente**
	- L'ambiente contiene **pi√π agenti**, e la distinzione chiave √® se il comportamento di un'altra entit√† pu√≤ essere descritto come il tentativo di **massimizzare una misura di prestazione** il cui valore dipende dalle azioni del tuo agente.

### Prevedibilit√† del Modello di Transizione
- **Deterministico**
	- Lo stato successivo dell'ambiente √® **completamente determinato** dallo stato corrente e dall'azione dell'agente (o degli agenti).
- **Stocastico**
	- Il modello dell'ambiente √® associato esplicitamente a **probabilit√†** per i risultati delle azioni (es. "c'√® una probabilit√† del 25% che domani piova").

- **Non Deterministico**
	- Lo stato successivo non √® completamente determinato, e le varie possibilit√† di risultato sono elencate **senza essere quantificate** (es. "c'√® la possibilit√† che domani piova").
### Struttura del Ciclo di Interazione
- **Episodico**
	- L'esperienza dell'agente √® divisa in episodi atomici. Ogni decisione √® **indipendente** dalle azioni intraprese negli episodi precedenti.

- **Sequenziale**
	- Ogni decisione pu√≤ **influenzare tutte le decisioni successive**. Le azioni a breve termine hanno conseguenze a lungo termine (es. gli scacchi, guidare).
### Cambiamento Temporale
- **Statico**
	- L'ambiente **non cambia** mentre l'agente sta decidendo come agire.

- **Dinamico**
	- L'ambiente **pu√≤ cambiare** mentre l'agente sta decidendo. Richiede che l'agente osservi continuamente e risponda rapidamente (es. un taxi autonomo).

- **Semi-Dinamico**
	- L'ambiente in s√© **non cambia** col tempo, ma la **misura di prestazione** (la valutazione) dell'agente s√¨ (es. scacchi giocati con l'orologio).
### Natura delle Variabili
- **Discreto**
	- Lo stato dell'ambiente, la gestione del tempo, le percezioni e le azioni sono rappresentabili con un **numero finito** di valori (es. le caselle su una scacchiera, input digitali).

- **Continuo**
	- Le variabili (stato, tempo, azioni) sono descritte da **numeri reali** e possono assumere un numero infinito di valori (es. la velocit√† di un'auto, l'angolo di sterzo).
### Visibilit√†
- **Ambiente Noto (Known)**
	- L'agente (o il suo progettista) **conosce le regole del gioco**. Sono noti i risultati (o le probabilit√† di risultato, se l'ambiente √® stocastico) per tutte le azioni.
	- L'agente pu√≤ eseguire una **ricerca _offline_** (pianificazione) per calcolare la sequenza di azioni ottimali prima di agire.

- **Ambiente Ignoto (Unknown)**
	- L'agente **non conosce le regole del gioco**. L'agente non sa come l'ambiente reagir√† alle sue azioni.
	- L'agente **dovr√† apprendere come funziona** l'ambiente. Dovr√† compiere **azioni esplorative** (sperimentazione) per acquisire la conoscenza dinamica necessaria a prendere buone decisioni.
#### ESEMPI
![Pasted image 20251016094914.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016094914.png)

## Ambiente simulato
In un sistema automatizzato o simulato, l‚Äôambiente **non √® reale**, ma **modellato da un software** che ne gestisce gli stati e le regole di funzionamento.
Il software che simula l‚Äôambiente deve ricreare l‚Äôintero **ciclo percezione‚Äìazione‚Äìvalutazione**.  
Per farlo, svolge una serie di funzioni fondamentali:
- generare stimoli per gli agenti
- raccogliere le azioni in risposta
- aggiornare il proprio stato
- attivare altri processi implicati dal cambiamento effettuato
- valutare le prestazioni degli agenti
#### Esempio di **ambiente di simulazione** che serve per valutare uno o pi√π **agenti**.
![Pasted image 20251016140906.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016140906.jpg)
## STRUTTURA DI UN AGENTE
$$ AGENTE=ARCHITETTURA + PROGRAMMA$$
- l'agente ha una sua funzione ( come spiegato in precedenza)
- `Agent()`
$$Agent:Percezioni \rightarrow Azioni$$
##### Pseudo programma agente
![Pasted image 20251016141848.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016141848.jpg)

# Diverse architetture di agenti
- <u><font color="#4bacc6">Basata su tabella</font></u>
	- Ogni azione dell'agente viene decisa in base a una **tabella che associa un'azione ad ogni possibile sequenza di percezioni**.  
	- gli *agenti reattivi semplici* usano questo tipo di architettura
![Pasted image 20251016143808.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016143808.jpg)
Partendo dall'ambiente, l'agente
- riceve delle percezioni tramite i sensori
	- capisce lo stato dell'ambiente
- guarda nella sua tabella (percezioni -> azioni)
	- esegue l'azione  
![Pasted image 20251016144146.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016144146.jpg)


- <u><font color="#4bacc6">Basata su modello</font></u>
- Gli agenti che usano questa architettura hanno una **memoria interna** che gli permette di rappresentare il mondo in cui si trovano. questi **mantengono e aggiornano uno stato interno** che descrive _cosa credono che stia succedendo_ 
![Pasted image 20251016144855.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016144855.jpg)
üîπ 1Ô∏è‚É£ Percezione e costruzione del modello del mondo
L‚Äôagente:
- **riceve le percezioni** dall‚Äôambiente attraverso i **sensori**;
- elabora da queste informazioni la rappresentazione di **‚Äúwhat the world is like now‚Äù**, cio√® com‚Äô√® il mondo in questo momento;
- **aggiorna lo stato interno** (la sua memoria) combinando:
    - le percezioni **attuali**,
    - le **percezioni passate**,
    - la conoscenza di **come il mondo evolve naturalmente** nel tempo (‚Äúhow the world evolves‚Äù),
    - e la conoscenza di **come le proprie azioni influenzano il mondo** (‚Äúwhat my actions do‚Äù).
üîπ 2Ô∏è‚É£ Predizione e decisione
Una volta aggiornato il proprio stato interno, l‚Äôagente:
- utilizza il modello del mondo per **prevedere** cosa accadr√† dopo e per valutare le possibili conseguenze delle sue azioni;
- decide **‚Äúwhat action I should do now‚Äù**, ossia l‚Äôazione pi√π opportuna da eseguire in base:
    - al proprio stato interno aggiornato,
    - alle **regole condizionali azione‚Äìcondizione** (_condition‚Äìaction rules_),
    - e alla **misura di prestazione** (il criterio con cui il progettista valuta l‚Äôefficacia del comportamento).
üîπ 3Ô∏è‚É£ Esecuzione dell‚Äôazione
Infine:
- l‚Äôagente invia l‚Äôazione scelta agli **attuatori**,
- che la eseguono sull‚Äôambiente, modificandone lo stato.

üëâ A questo punto il ciclo ricomincia: l‚Äôambiente cambia, genera nuove percezioni e l‚Äôagente aggiorna di nuovo il proprio modello interno.

![Pasted image 20251016145402.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016145402.jpg)


- <u><font color="#4bacc6">Basata su obiettivo</font></u>
	- Gli **agenti basati su obiettivo** sono un‚Äôevoluzione degli agenti basati su modello.  
	- Come loro, **mantengono uno stato interno** del mondo (memoria e conoscenza di come si evolve), 
	- ma in pi√π **hanno un obiettivo da raggiungere (goal)** che guida la scelta delle azioni.
![Pasted image 20251016145830.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016145830.jpg)
L‚Äô**agente basato su obiettivo**, **va oltre**:  
‚Üí non si limita a prevedere _cosa succeder√†_, ma **decide cosa vuole che succeda**, e **sceglie le azioni** per raggiungere un _goal_ (obiettivo) desiderato.

- <u><font color="#4bacc6">Basato su utilit√†</font></u>
Gli **agenti con valutazione di utilit√†** sono un‚Äôestensione degli agenti basati su obiettivo.  
Anzich√© limitarsi a _raggiungere un goal_, valutano **quanto √® ‚Äúbuono‚Äù o vantaggioso** ciascun possibile stato del mondo.
- utilizzano una funzione di utilit√†
	- √à una **funzione che assegna a ogni stato un valore numerico** ‚Üí rappresenta **quanto l‚Äôagente √® soddisfatto** in quello stato (‚Äúquanto sar√≤ felice se arrivo l√¨‚Äù).
$$U(s) = \text{grado di utilit√† dello stato }$$

- <u><font color="#4bacc6">Basata su apprendimento</font></u>
	- Questi agenti sono in grado di **migliorare il proprio comportamento nel tempo**, grazie a un meccanismo di apprendimento interno.
![Pasted image 20251016150117.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016150117.jpg)
- üîπ **Performance Element**
	- √à il **cuore operativo** dell‚Äôagente:
	- Riceve **le percezioni** dai sensori.
	- Sceglie **le azioni** da eseguire tramite gli attuatori.
	- √à la parte che determina **‚Äúcome si comporta l‚Äôagente ora‚Äù**.

- üîπ **Critic**
	- Valuta la qualit√† delle azioni dell‚Äôagente.
	- Confronta le prestazioni osservate con un **performance standard** (criterio di riferimento) e fornisce **feedback**.
	- Usa le percezioni ricevute dai sensori per osservare gli effetti delle azioni.
	- Produce un segnale di feedback (positivo o negativo) che dice all‚Äôagente quanto bene ha agito.

- üîπ **Learning Element**
	- Il **modulo di apprendimento** utilizza il feedback del _critic_ per **migliorare il comportamento futuro** dell‚Äôagente.
	- Aggiorna il _performance element_ (cio√® modifica il modo in cui l‚Äôagente prende decisioni).

- üîπ **Problem Generator**
	- Suggerisce **nuove azioni da provare** per ottenere esperienze utili all‚Äôapprendimento.

## Rappresentazione degli stati
Quando un agente deve ragionare o apprendere, ha bisogno di **una rappresentazione interna dello stato del mondo**.  

##### 1. Rappresentazione atomica
![Pasted image 20251016150129.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016150129.jpg)
- Ogni **stato** o **situazione** √® considerato come un **blocco unico e indivisibile**.
- L‚Äôagente conosce solo _che quello stato esiste_, ma **non ha informazioni sulla sua struttura interna**.
- √à il modello pi√π semplice:
	‚Üí **stati finiti**, **transizioni semplici**, a volte con **probabilit√† associate** (se stocastico).

##### 2. Rappresentazione fattorizzata
![Pasted image 20251016150349.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016150349.jpg)
- Ogni stato √® **descritto tramite un insieme di variabili (fattori)**.
- Invece di trattare tutto come un unico blocco, l‚Äôagente **rappresenta le caratteristiche principali** dello stato (es. posizione, temperatura, velocit√†, ecc.).
- Queste variabili possono essere viste come **dimensioni in uno spazio vettoriale**.

##### 3. Rappresentazione strutturata
![Pasted image 20251016150400.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251016150400.jpg)
- √à la pi√π **ricca e complessa**.
- Gli oggetti non sono solo elenchi di valori, ma **entit√† con relazioni tra loro** (come in un grafo o in un linguaggio logico).
- Permette di descrivere **relazioni, gerarchie e dipendenze**.

LEZ.3 -> 

---
## AGENTI RISOLUTORI DI PROBLEMI
- si basano sull'architettura basata sui goal
	- Tipologia di agenti che puntano a risolvere un problema attraverso un algoritmo ben definito
		- Di solito usano algoritmi di ricerca
			- che possono essere di tipo:
				- *informato*
					- L‚Äôagente stima quanto √® vicino al goal, riducendo la ricerca
						- Greedy Search, A*
				- *non informato*
					- L‚Äôagente non conosce la distanza dal goal, esplora tutto lo spazio
						- BFS, DFS, Uniform Cost
	- l‚Äôagente risolutore di problemi **costruisce prima un piano completo di azioni**, tramite un **processo di ricerca interna**.
#### üß© Rappresentazione e categorie
- Gli agenti risolutori di problemi utilizzano rappresentazioni **atomiche**,  
    dove gli **stati del mondo** sono **entit√† indivisibili** (nodi di un grafo) **senza struttura interna visibile**.
- Gli agenti che invece utilizzano **rappresentazioni fattorizzate o strutturate** (cio√® con sottocomponenti interne agli stati) sono detti **agenti pianificatori**.
- Il **ragionamento dell‚Äôagente atomico** √® puramente **algoritmico** e basato su **modelli di grafo** (stati = nodi, azioni = archi).
Gli agenti risolutori di problemi operano in ambienti **semplici e controllabili**, tipicamente:
- Episodici
- A singolo agente
- **Completamente osservabili**
- **Deterministici**
- **Statici** (non cambiano mentre l‚Äôagente pensa)
- **Discreti** (stati e azioni finite)
- **Noti** (modello di transizione conosciuto)
####  Esecuzione: anello aperto vs anello chiuso
- In un **ambiente completamente osservabile, deterministico e noto**, la soluzione √® una **sequenza fissa di azioni**.  
    ‚Üí L‚Äôagente pu√≤ **ignorare le percezioni durante l‚Äôesecuzione**:  
    si parla di **sistema ad anello aperto (open-loop)**.
- Se invece:
    - il modello pu√≤ essere impreciso, oppure
    - l‚Äôambiente non √® deterministico,  
        ‚Üí l‚Äôagente deve **monitorare le percezioni e riadattarsi**,  
        quindi lavora in **anello chiuso (closed-loop)**.
#### ‚öôÔ∏è  Le quattro fasi principali del processo di risoluzione

| Fase                               | Descrizione                                         |
| ---------------------------------- | --------------------------------------------------- |
| **1. Formulazione dell‚Äôobiettivo** | L‚Äôagente decide cosa vuole raggiungere              |
| **2. Formulazione del problema**   | Definisce stati, azioni, transizioni e costi        |
| **3. Ricerca (Search)**            | Calcola una sequenza ottimale di azioni nel modello |
| **4. Esecuzione (Execution)**      | Esegue il piano nel mondo reale                     |

üìå Durante la **ricerca**, l‚Äôagente non agisce fisicamente: pensa, simula e valuta internamente.
## DEFINIZIONE FORMALE DEL PROBLEMA DI RICERCA
Un **problema di ricerca** √® una **descrizione astratta** di una situazione in cui un agente deve **trovare una sequenza di azioni** che porti dallo **stato iniziale** a uno **stato obiettivo**.
$$\text{Problema di ricerca} = \langle S, S_0, A, Result, Goal, C \rangle$$
Un problema di ricerca √® definito da **cinque elementi principali**:

|#|Componente|Descrizione|
|---|---|---|
|**1Ô∏è‚É£**|**Stato iniziale**|√à lo stato in cui si trova l‚Äôagente all‚Äôinizio del problema.|
|**2Ô∏è‚É£**|**Azioni possibili**|La funzione **Azioni(s)** restituisce l‚Äôinsieme finito di azioni eseguibili nello stato `s`. Ogni azione √® _applicabile_ in `s`.  <br>Es: `Azioni(Arad) = {VersoSibiu, VersoTimisoara, VersoZerind}`|
|**3Ô∏è‚É£**|**Modello di transizione**|Descrive come le azioni modificano lo stato del mondo.  <br>Formalmente: `Risultato(s, a) = s‚Ä≤` indica lo stato successivo ottenuto eseguendo l‚Äôazione `a` nello stato `s`.  <br>Es: `Risultato(Arad, VersoZerind) = Zerind`|
|**4Ô∏è‚É£**|**Insieme di stati obiettivo**|Contiene uno o pi√π stati che soddisfano il goal dell‚Äôagente (le condizioni di successo).|
|**5Ô∏è‚É£**|**Funzione di costo**|La funzione `CostoAzione(s, a, s‚Ä≤)` (o `c(s, a, s‚Ä≤)`) assegna un valore numerico positivo al costo di eseguire `a` in `s` per raggiungere `s‚Ä≤`.  <br>Serve per confrontare soluzioni e trovare quella pi√π economica.|
- Una **sequenza di azioni** forma un **cammino** (_path_) attraverso lo spazio degli stati.
- Una **soluzione** √® un cammino che parte dallo stato iniziale e arriva a uno stato obiettivo.
- Una **soluzione ottima** √® quella con **costo totale minimo**, rispetto alla funzione di costo definita.
- Lo **spazio degli stati** pu√≤ essere rappresentato come un **grafo**:
    - i **nodi** (vertici) rappresentano gli **stati**;
    - gli **archi orientati** rappresentano le **azioni** e le **transizioni**;
    - i **pesi degli archi** rappresentano i **costi delle azioni**.
#### Modello e Tipi di astrazione
Quando formuliamo un problema (es. ‚Äú**raggiungere una certa citt√†**‚Äù), stiamo creando un **modello**,  
cio√® una **descrizione matematica astratta** della realt√†.  
Non rappresentiamo il mondo in tutti i suoi dettagli, ma solo gli aspetti **rilevanti per la risoluzione del problema**.
Il processo con cui **semplifichiamo una rappresentazione** eliminando dettagli non essenziali  
√® chiamato **astrazione**.


|Tipo di astrazione|Definizione|Utilit√†|
|---|---|---|
|**Astrazione Valida**|Ogni soluzione trovata nel modello astratto pu√≤ essere **espansa** in una soluzione valida nel mondo reale pi√π dettagliato.|Garantisce che la soluzione ‚Äúastratta‚Äù sia **corretta e applicabile** nella realt√†.|
|**Astrazione Utile**|Le azioni nella soluzione astratta sono **pi√π facili o pi√π economiche** da eseguire rispetto a quelle nel problema reale.|Permette di **semplificare la ricerca** e ridurre il costo computazionale.|

üìå Una buona astrazione √® **sia valida che utile**.
 
### PROBLEMI ESEMPLIFICATIVI E REALI
Lo studio dei **problemi di ricerca** avviene partendo da **problemi esemplificativi (standardizzati)**,  
che servono a **testare metodi di risoluzione**, per poi passare a **problemi reali**,  
pi√π complessi e legati a contesti pratici.

- I problemi **esemplificativi**:
	- Servono per **illustrare o mettere alla prova diversi metodi di risoluzione** (ricerca, ottimizzazione, pianificazione, ecc.).
	- Sono **astratti**, **semplificati** e **standardizzati**, cio√® formulati in modo generico per poter essere applicati a diversi algoritmi.
- Invece, i **problemi reali**:
    - Hanno una formulazione specifica e non standard,
    - e le soluzioni trovate hanno **utilit√† pratica**.
###### üî∏ Problemi ESEMPLIFICATIVI
###### üßπ Esempio: Mondo dell‚ÄôAspirapolvere
Uno dei problemi classici su griglia, usato per testare gli algoritmi di ricerca di base.
![Pasted image 20251021125057.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021125057.jpg)
![Pasted image 20251021125109.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021125109.jpg)


|Elemento|Descrizione|
|---|---|
|**Stati**|Ogni stato indica la posizione dell‚Äôagente e la presenza o assenza di sporco in ogni cella.  <br>In un mondo con `n` celle, ci sono `n √ó 2‚Åø` stati possibili.|
|**Stato iniziale**|Pu√≤ essere qualunque configurazione iniziale di agente e sporco.|
|**Azioni**|`Sinistra`, `Destra`, `Aspira` (nel mondo a due celle).|
|**Modello di transizione**|`Aspira` rimuove lo sporco dalla cella; `Sinistra` e `Destra` spostano l‚Äôagente (se non ci sono muri).|
|**Stati obiettivo**|Stati in cui **tutte le celle sono pulite**.|
|**Costo di azione**|Tutte le azioni hanno **costo uniforme = 1**.|

üìå Lo **spazio degli stati** √® un piccolo grafo, gestibile, utile per illustrare i concetti di esplorazione e soluzione ottimale.

---

###### üß© Puzzle dell‚ÄôOtto

Un classico problema di ricerca usato per confrontare algoritmi come A*, BFS, DFS, ecc.
![Pasted image 20251021132649.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021132649.jpg)

|Elemento|Descrizione|
|---|---|
|**Stati**|Tutte le possibili configurazioni della scacchiera 3√ó3 con i numeri da 1 a 8 e una casella vuota.|
|**Stato iniziale**|Una configurazione specifica del puzzle.|
|**Obiettivo**|Configurazione ordinata (numeri da 1 a 8, casella vuota in basso a destra).|
|**Azioni**|Spostare la casella vuota **su, gi√π, destra, sinistra**.|
|**Goal test**|Verificare se la configurazione corrente corrisponde allo stato obiettivo.|
|**Costo del cammino**|Costo uniforme (ogni mossa = 1).|
|**Spazio degli stati**|Molto ampio, pu√≤ contenere cicli; adatto a testare efficienza degli algoritmi.|

###### üëë Problema delle Otto Regine
![Pasted image 20251021132907.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021132907.jpg)

Un altro problema classico per testare **formulazioni diverse** e strategie di ricerca.

|Formulazione|Descrizione|Spazio di ricerca|
|---|---|---|
|**Incrementale 1 (base)**|Si aggiungono regine una per volta su qualunque casella.|~1.8 √ó 10¬π‚Å¥ sequenze (molto grande).|
|**Incrementale 2 (migliorata)**|Si aggiunge una regina per colonna, assicurandosi che non minacci le precedenti.|Solo 2057 stati (molto pi√π efficiente).|
|**A stato completo**|La scacchiera contiene 8 regine (una per colonna) e si spostano finch√© non sono tutte non minacciate.|Usata in algoritmi di ricerca locale (es. _Hill Climbing_).|

üìå Questo problema mostra come **la formulazione influenza l‚Äôefficienza della ricerca**:  
pi√π il modello √® compatto e vincolato, pi√π facile sar√† trovare una soluzione.


######  Problemi del mondo reale

I problemi reali sono molto pi√π **complessi** dei modelli astratti e le loro soluzioni sono **praticamente utili**.  
Spesso la loro formulazione √® **specifica e non standardizzata**.
‚úàÔ∏è Esempio: Problema di ricerca dell‚Äôitinerario aereo

| Elemento                   | Descrizione                                                                                                         |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| **Stati**                  | Includono posizione (aeroporto), ora corrente e altre informazioni storiche (es. tratte, tariffe, voli precedenti). |
| **Stato iniziale**         | L‚Äôaeroporto di partenza dell‚Äôutente.                                                                                |
| **Azioni**                 | Prendere un volo disponibile dopo l‚Äôora corrente, rispettando i tempi di trasferimento.                             |
| **Modello di transizione** | Lo stato successivo aggiorna la posizione e l‚Äôorario di arrivo del volo.                                            |
| **Stato obiettivo**        | Aeroporto di destinazione desiderato.                                                                               |
| **Costo dell‚Äôazione**      | Combinazione di fattori: costo del biglietto, tempo, durata, coincidenze, dogane, qualit√† del posto, ecc.           |

# üîé Algoritmi di Ricerca

Un **algoritmo di ricerca** riceve in input un **problema di ricerca** e restituisce una **soluzione** (un cammino verso lo stato obiettivo) o un **fallimento**.
- Gli algoritmi costruiscono un **albero di ricerca** sul **grafo dello spazio degli stati**.
- Ogni **nodo** rappresenta uno **stato**, ogni **ramo** un‚Äô**azione**.
- **nodo** dell‚Äôalbero contiene:
	- `n.stato` ‚Üí lo stato rappresentato
	- `n.padre` ‚Üí il nodo da cui √® stato generato
	- `n.azione` ‚Üí l‚Äôazione che ha portato a questo stato
	- `n.costo-cammino = g(n)` ‚Üí costo totale dal nodo iniziale fino a `n`
- La **radice** corrisponde allo stato iniziale.
	- Espandere un nodo = generare i **nodi figli** applicando le azioni possibili (`Risultato(s, a)`).

üìå **Distinzione chiave:**

- **Spazio degli stati** ‚Üí tutti i possibili stati del mondo.

- **Albero di ricerca** ‚Üí cammini esplorati dall‚Äôagente durante la ricerca.

- La **frontiera** √® l‚Äôinsieme dei nodi generati ma non ancora espansi.
	- Separa gli **stati esplorati** (interni) da quelli **ancora da esplorare** (esterni).
	- √à implementata come una **coda**(FIFO,LIFO,PRIOR) , con le operazioni:
		- `VUOTA?(coda)` ‚Üí verifica se la frontiera √® vuota
		- `POP(coda)` ‚Üí estrae un nodo dalla frontiera
		- `INSERISCI(elemento, coda)` ‚Üí aggiunge nuovi nodi (figli)
- La **strategia di scelta del nodo da espandere** determina il tipo di algoritmo (BFS, DFS, A*, ecc.).
- *tipi di misura di prestazioni:*

| Criterio        | Descrizione                               |
| --------------- | ----------------------------------------- |
| **Completezza** | Trova una soluzione se esiste.            |
| **Ottimalit√†**  | Restituisce la soluzione di costo minimo. |
| **Tempo**       | Numero di nodi generati.                  |
| **Spazio**      | Memoria richiesta.                        |

$$\text{Costo totale} = \text{Costo della ricerca} + \text{Costo del cammino soluzione}$$

L‚Äôobiettivo √® **minimizzare il costo complessivo**: trovare una soluzione **valida e conveniente** con il minor sforzo possibile.

#### üßÆ Tipi di Gestione della Frontiera (strategie)

|Strategia|Struttura dati|Comportamento|
|---|---|---|
|**FIFO**|Coda|Ricerca in ampiezza (Breadth-First Search)|
|**LIFO**|Pila|Ricerca in profondit√† (Depth-First Search)|
|**Coda con priorit√†**|Ordinata da una funzione di costo o euristica|Ricerca di costo uniforme, Greedy, A*|

##### üîπ Tipi di strategie

###### üî∏ Non informate (cieche)

- Non usano informazioni sul goal.  
    Esempi:
    - **Ricerca in ampiezza (BFS)**
    - **Ricerca di costo uniforme (UCS)**
    - **Ricerca in profondit√† (DFS)**
    - **Profondit√† limitata**
    - **Approfondimento iterativo**
###### üî∏ Informate (euristiche)
- Usano una **funzione euristica** `h(n)` che stima la distanza dal goal.  
    Esempi:
    - **Greedy Search**
    - **A*** (ricerca ottimale euristica)

üìå Ogni strategia cerca un equilibrio tra **tempo**, **spazio**, **completezza** e **ottimalit√†**.

#### Pseudocodice generico di ricerca albero
![Pasted image 20251021140935.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021140935.jpg)

#### Pseudocodice pi√π dettagliato
![Pasted image 20251021141804.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021141804.jpg)


#### Ricerca in ampiezza
![Pasted image 20251021142331.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021142331.jpg)
##### Pseudo
![Pasted image 20251021142416.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021142416.jpg)
- √à una **ricerca non informata** e **sistematica**, completa anche su spazi di stati infiniti (se ogni stato ha un numero finito di successori).
- Usa una **coda FIFO**: i nuovi nodi vengono aggiunti in fondo, e quelli pi√π vecchi vengono espansi per primi.

| Aspetto            | Descrizione                                                                              |
| ------------------ | ---------------------------------------------------------------------------------------- |
| **Frontiera**      | Implementata come coda FIFO                                                              |
| **Test obiettivo** | Pu√≤ essere effettuato subito dopo la generazione di un nodo (‚Äúanticipato‚Äù)               |
| **Raggiunti**      | Insieme degli stati gi√† visitati, per evitare di riespandere stati gi√† esplorati         |
| **Espansione**     | Tutti i nodi di profondit√† $d$ vengono generati **prima** di quelli a profondit√† $d + 1$ |

#####  Propriet√† della BFS

| Propriet√†                        | Valore                                                   |
| -------------------------------- | -------------------------------------------------------- |
| **Completezza**                  | ‚úÖ Sempre completa (trova una soluzione se esiste)        |
| **Ottimalit√†**                   | ‚úÖ Ottimale se **tutti i costi delle azioni sono uguali** |
| **Tempo**                        | $O(b^d)$                                                 |
| **Spazio**                       | $O(b^d)$                                                 |
| **Fattore di ramificazione (b)** | Numero massimo di successori di un nodo                  |
| **Profondit√† (d)**               | Profondit√† della soluzione pi√π superficiale              |
| **Cammino massimo (m)**          | Lunghezza massima di un cammino nello spazio di ricerca  |
##### Complessit√†
Se tutti gli operatori hanno costo costante $k$:
$g(n) = k \times \text{profondit√†}$
- **Complessit√† temporale:**
    $$T(b, d) = b + b^2 + \dots + b^d = O(b^d)$$
- **Complessit√† spaziale:**  
    Anch‚Äôessa $$O(b^d)$$ poich√© tutti i nodi devono essere mantenuti in memoria.
üìå Entrambe le complessit√† sono **esponenziali**, quindi la BFS √® praticabile solo per problemi di piccola scala.
#### Ricerca a Costo Uniforme (Uniform-Cost Search, UC)
- √à una **generalizzazione della ricerca in ampiezza**, usata quando **le azioni hanno costi diversi**
- L‚Äôidea √® di **espandere sempre il nodo con costo di cammino minimo** $g(n)$
- Usa una **coda con priorit√†** come frontiera (invece della coda FIFO della BFS).
- Espande i nodi **in ordine di costo crescente**
- Si comporta come l‚Äô**algoritmo di Dijkstra**: la ricerca si ‚Äúespande a onde‚Äù di costo uniforme
![Pasted image 20251021143052.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021143052.jpg)
## Propriet√†

| Propriet√†       | Descrizione                                                                 |
| --------------- | --------------------------------------------------------------------------- |
| **Completezza** | ‚úÖ Completa se $\varepsilon > 0$ (cio√® se ogni azione ha un costo positivo). |
| **Ottimalit√†**  | ‚úÖ Ottimale rispetto al costo del cammino.                                   |
| **Tempo**       | $O(b^{1 + \lfloor C^*/\varepsilon \rfloor})O$                               |
| **Spazio**      | $O(b^{1 + \lfloor C^*/\varepsilon \rfloor})O$                               |

dove:

- $C^*$ = costo della soluzione ottima
- $\varepsilon$ = costo minimo possibile per un‚Äôazione

üìå L‚Äôesponente $1 + \lfloor C^*/\varepsilon \rfloor$rappresenta la **profondit√† massima** che deve essere esplorata per garantire l‚Äôottimalit√†, includendo anche il livello iniziale.
##### üí° Confronto con la Ricerca in Ampiezza

|Caso|Comportamento|
|---|---|
|**Tutti i costi uguali**|UC ‚â° BFS (stesse prestazioni e soluzione minima in numero di azioni).|
|**Costi diversi**|UC esplora prima i cammini a costo minore, garantendo la soluzione pi√π economica.|

#### Ricerca in profondit√† DFS
![Pasted image 20251021151954.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021151954.png)
#### Analisi costi
‚Ä¢ Se m distanza massima della soluzione nello spazio di ricerca 
‚Ä¢ b fattore di diramazione 
	‚Ä¢ Allora la complessit√† temporale √®: $O(b^{m+1})$
	- spazio $O(b*m+1)$ 
profondit√† usa meno memoria di ampiezza
### Ricerca in profondit√† limitata
La Ricerca in Profondit√† Limitata √® una strategia di ricerca non informata che esegue la ricerca in profondit√† fino a un livello massimo predefinito, chiamato **limite di profondit√† ($\ell$)**.
- **Principio Operativo:** La ricerca procede in profondit√†, espandendo il nodo pi√π recente, ma si ferma non appena si raggiunge il livello $\ell$.
- **Limite $\ell$:** Questo valore predefinito agisce come un "muro" o un vincolo; i nodi al livello $\ell$ non vengono espansi, evitando cos√¨ che la ricerca si perda indefinitamente in rami profondi o cicli.
- **Esempio di Utilizzo:** √à utile per problemi in cui si conosce un **limite superiore** per la profondit√† della soluzione (es. in un problema di _Route-finding_ tra $N$ citt√†, la soluzione pi√π lunga non pu√≤ superare $\text{N}-1$ mosse).
‚Ä¢ Complessit√† tempo: $O(b^d)$
- Spazio: $O(b*d)$
![Pasted image 20251021152344.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021152344.png)
![Pasted image 20251021152421.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021152421.png)

#### Ricerca Bidirezionale

La **ricerca bidirezionale** esplora simultaneamente:
- **In avanti** dallo **stato iniziale**, e
- **All‚Äôindietro** dallo **stato obiettivo**,
fino a far **incontrare le due ricerche** in un punto intermedio dello spazio degli stati.
- Invece di esplorare $O(b^d)$ nodi, ne esplora circa:
    $O(b^{d/2} + b^{d/2}) \approx O(b^{d/2})$
- Questo riduce drasticamente il numero di nodi da analizzare, **a parit√† di profondit√†** $d$.
üìå Funziona bene solo se:
- √à possibile **ragionare all‚Äôindietro** (cio√® generare predecessori),
- E lo **stato obiettivo** √® **ben definito**.
- Mantiene **due frontiere** (una per ogni direzione) e due insiemi di **stati raggiunti**.
- Espande **il nodo con costo minore** tra i due lati (strategia best-first).
- Se la funzione di valutazione √® il **costo di cammino**, otteniamo una **ricerca bidirezionale a costo uniforme**, ottimale come UC.
- Nessun nodo con costo >$C^*/2$ (dove $C^‚àó$ √® il costo ottimo) viene espanso.
![Pasted image 20251021154403.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021154403.png)

#### TUTTE LE STRATEGIE A CONFRONTO
![Pasted image 20251021154529.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021154529.png)

#### PROBLEMA DEI CICLI
##### Tre soluzioni pratiche
![Pasted image 20251021154940.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021154940.png)

##### Esempio di soluzione con i grafi
![Pasted image 20251021155035.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021155035.png)

### Fix della ricerca-grafo in ampiezza
![Pasted image 20251021155104.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021155104.png)
### Fix della ricerca-grafo con costo uniforme UC
![Pasted image 20251021155139.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251021155139.png)

# Ricerca esaustiva
Nella ricerca **non informata**, come BFS o UC, l‚Äôagente esplora tutto lo spazio degli stati ‚Äúalla cieca‚Äù, senza sapere quale strada lo avvicina davvero alla soluzione.  
	‚Üí Questo √® **impraticabile** quando il numero di stati cresce in modo **esponenziale**.
- si cerca quindi di applicare della euristica
	- Una **euristica** √® una _stima intelligente_ della distanza (o del costo) che manca per raggiungere l‚Äôobiettivo.
		- deriva da **esperienza o conoscenza del dominio**
		- esplora prima i nodi pi√π promettenti.
		- non garantisce l‚Äôottimo assoluto, ma una **buona soluzione in tempi accettabili**.
- Funzione di valutazione euristica
	- La conoscenza euristica si formalizza in una **funzione**:
$$f: n \rightarrow \mathbb{R}
$$
	- Questa associa a ogni **nodo n** (che rappresenta uno stato) un **valore numerico** che misura ‚Äúquanto sembra promettente‚Äù quel nodo.
		- $f(n)$ √® un **numero reale**;
		- si calcola **a partire dallo stato del nodo (`n.Stato`)**, non dalla sua storia.

### Ricerca Best-First
![Pasted image 20251025113910.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025113910.png)
Ad ogni passo viene scelto il **nodo pi√π promettente**, ossia quello con il **valore di f(n) pi√π basso** (in caso di costi o distanze).
- ‚ÄúMigliore = minore‚Äù, perch√© un f(n) piccolo indica ‚Äúpi√π vicino‚Äù al goal.
- L‚Äôimplementazione usa una **coda con priorit√†**, ordinata in base al valore di f(n).
- Quindi tutto dipende da **come definiamo f(n)**
	- in quello classico lo definiamo uguale a $g(n)$
#### CLASSICO
![Pasted image 20251025111757.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025111757.png)
- **g(n)** = costo reale del cammino dall‚Äôinizio al nodo n.
- Nessuna euristica: usiamo solo il costo accumulato finora.
In questo caso, la ‚ÄúBest-First‚Äù **coincide esattamente con la Ricerca di Costo Uniforme (Uniform-Cost Search)**.
- **`Goal-Test`** √® una funzione fornita dalla definizione del problema:  
	- prende come input lo **stato** di un nodo e restituisce **vero** se quello stato √® uno degli **stati obiettivo** (cio√® soddisfa la condizione del goal), oppure **falso** in caso contrario.
Infatti:
- l‚Äôalgoritmo sceglie sempre il nodo con il **costo cumulativo minore** (`lowest-cost node in frontier`),
- e continua ad espandere fino a trovare il goal con costo minimo.
#### Greedy Best-First
![Pasted image 20251025112102.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025112102.png)
![Pasted image 20251025111949.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025111949.png)
- sfrutta **h(n)**
	- **h(n)** = stima del costo _dal nodo n al goal_ (quanto ‚Äúmanca‚Äù).
		- non √® proprio euristica perch√© vede solo il passato
- L‚Äôalgoritmo ignora completamente il costo gi√† speso **(g(n))**, guarda solo _chi sembra pi√π vicino alla meta_.
**Vantaggi:**
- molto pi√π veloce, perch√© guida subito verso il goal.  
**Svantaggi:**
- non √® ottimale (pu√≤ trovare percorsi pi√π costosi).   
- non √® garantito che sia completa se ci sono cicli o stime sbagliate.


LEZ.4 ->

---
# A* search
L'idea principale √® cercare un equilibrio tra:
- **arrivare al goal** (come Greedy),
- **risparmiare sul costo fatto finora** (come Uniform Cost),  
Quindi l'alg, vuole **trovare il percorso totale pi√π economico possibile**, stimando il costo complessivo.
### La funzione di valutazione
A usa:$$
f(n)=g(n)+h(n)$$
dove:
- **g(n)** = costo _reale_ per arrivare fino al nodo `n` (gi√† percorso);
- **h(n)** = stima _euristica_ del costo rimanente per arrivare al goal;
- quindi **f(n)** = stima del costo _totale_ del cammino passando per n.

In altre parole:
> A valuta ogni nodo come ‚Äúquanto ho speso finora + quanto (credo) manchi ancora‚Äù.

- se vogliamo che un algoritmo A sia completo 
Condizione sufficiente: ogni passo deve costare almeno **Œµ > 0**, cio√®
$$g(n) ‚â• d(n)¬∑Œµ$$
‚Üí garantisce che l‚Äôalgoritmo non resti bloccato in cicli o cammini infiniti a costo zero.
####  L‚Äôalgoritmo A*
- per fare chiarezza, quando si parla di A, si intende la famiglia di algoritmi che usa la funzione di valutazione $f(n)=g(n)+h(n)$
- quando si parla di A* si intende
L‚Äô**$A^*$** √® un **caso particolare di A**, in cui si **impone una condizione precisa sull‚Äôeuristica h(n)** per fare in modo che sia completo e ottimale:  

| Propriet√†                  | Significato                                                         | Effetto                                              |
| -------------------------- | ------------------------------------------------------------------- | ---------------------------------------------------- |
| **h(goal)=0**              | al goal il costo mancante √® 0                                       | condizione base                                      |
| **h(n) ‚â• 0**               | nessuna stima negativa                                              | realismo                                             |
| **Ammissibile**            | non sovrastima mai il costo reale: $h(n) ‚â§ h^*(n)h$                 | garantisce ottimalit√†                                |
| **Consistente (Monotona)** | $h(n) ‚â§ c(n,a,n') + h(n')$           dove $n'$ √® un successore di n | evita ri-espansioni, assicura $f(n)$ non decrescente |
###### PSEUDOCODICE

```scss
function A* (problem) returns a solution or failure
nodo <- nodo con stato = problem.initialstate
frontiera <- coda di priorit√† ordinata in base a f(n) con all'inizio solo nodo "nodo"
esplorati <- insieme dei nodi esplorati inizialmente vuoto
loop do
    if frontiera is empty? then return failure
    nodo <- POP(frontiera)
    if problem.GOALTEST(nodo.state) then return SOLUTION(nodo)
    add nodo.state to esplorati
    for each action in problem.ACTIONS(nodo.state) do
        child <- CHILD-NODE(problem, nodo, action)
        if child.state non in frontiera or esplorati then
            frontiera <- INSERT(child.state)
        else if child.state is in frontiera con f(n) pi√π alto allora
            replace that frontier node with child
```

##### CODICE A*
##### Complessit√† e Limiti di A*

- **Tempo:** esponenziale $O(b^d)$
- **Spazio:** molto elevato $O(b^{d+1})$
- **Problema principale:** memoria ‚Üí A* mantiene tutta la frontiera.

#### Migliorare l'occupazione in memoria di A*

#### Mediante Beam Search
Invece di tenere in memoria TUTTI I NODI, l'idea √® quella di ricordare solo i *k nodi pi√π promettenti*, dove `k` √® detto **ampiezza del raggio (beam)**.

>[!tip] LA BEAM SEARCH ***NON* √à COMPLETA**

### Idea e pseudocodice
![Pasted image 20251025193001.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025193001.jpg)

![Pasted image 20251025193007.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025193007.jpg)


>[!tip]- Esempio
>![Pasted image 20251025193059.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025193059.jpg)
#### IDA*
L'algoritmo IDA* combina
- A*
- ricerca in profondit√† iterativa(con limite) (ID)
Pi√π precisamente, combina i vantaggi di entrambe
- come A*: usa la funzione di valutazione $$f(n) = g(n) + h(n)$$
- come ID: esplora **in profondit√†**, ma **con un limite**.
- il limite non sulla profondit√† ma sul valore del nodo
	- Si imposta un limite iniziale $f_{limit}$ ;
	- Si esplora in profondit√† solo i nodi con $f(n) ‚â§ f_{limit}$
	- Se non si trova la soluzione, si aumenta $f_{limit}$ e si ricomincia.
**Tempo**‚ùå esponenziale (come A*)
**Spazio**‚úÖ lineare, $O(b¬∑d)$
## ‚öôÔ∏è Valutazione delle Funzioni Euristiche

### üîπ Definizione

Una **funzione euristica** $h(n)$ stima il **costo rimanente** per raggiungere il goal da un nodo $n$.

> A parit√† di ammissibilit√†, un‚Äôeuristica √® **pi√π efficiente** quanto pi√π √® **informata**, cio√® quanto pi√π i suoi valori si avvicinano al costo reale $h^‚àó(n)$.

---

### üîπ Livello di informazione

| Caso                | Descrizione                          | Tipo di ricerca    |
| ------------------- | ------------------------------------ | ------------------ |
| $h(n) = 0$          | Nessuna informazione ‚Üí esplora tutto | BFS / Uniform Cost |
| $0 < h(n) < h^*(n)$ | Euristica ammissibile, ma parziale   | A*                 |
| $h(n) = h^*(n)$     | Conoscenza perfetta (oracolo)        | Ottimo immediato   |

Condizione generale per le euristiche **ammissibili**:
$0 \le h(n) \le h^*(n)$
#### Teorema di dominanza

> Se $h_1(n) \le h_2(n)$per ogni nodo $n$:
> 
> - i nodi espansi da **A*** con $h_2$‚Äã sono **un sottoinsieme** di quelli espansi con $h_1$;
>     
> - quindi **A*** con $h_2$‚Äã √® **almeno efficiente quanto** con $h_1$‚Äã.
>     

üìå In sintesi:
> Pi√π l‚Äôeuristica √® **vicina a $h^‚àó$, meno nodi vengono esplorati ‚Üí pi√π efficiente.

##### Compromesso: costo dell‚Äôeuristica vs costo della ricerca
- Un‚Äôeuristica **semplice** √® veloce da calcolare ma fa esplorare molti nodi.
- Un‚Äôeuristica **precisa** riduce la ricerca ma pu√≤ essere costosa da valutare.
> L‚Äôobiettivo √® trovare un equilibrio tra **qualit√† della stima** e **costo computazionale**
![Pasted image 20251025193638.jpg](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251025193638.jpg)
##### Misurare l‚Äôefficacia: Fattore di diramazione effettivo $b^*$
Per misurare quanto √® ‚Äúforte‚Äù o efficace un‚Äôeuristica possiamo utilizzare un valore chiamato **fattore di diramazione effettivo (`b*`)**.

Rappresenta **il numero medio di nodi generati per livello** dalla ricerca.

$N + 1 = 1 + b^* + (b^*)^2 + \dots + (b^*)^d$

Dove:
- $N$: numero totale di nodi generati
- $d$: profondit√† della soluzione
üí° Pi√π $b^*$ √® vicino a **1**, pi√π l‚Äôeuristica √® **efficace**.

#### Come inventare una euristica
- **Rilassamento del problema:** togli vincoli per ottenere un problema pi√π semplice ‚Üí la sua soluzione d√† una _sottostima_ ammissibile.
- **Massimizzazione:** se hai pi√π euristiche ammissibili, prendi il _massimo_ ‚Üí resta ammissibile e pi√π informata.
- **Sottoproblemi / Pattern database:** pre-calcola i costi di sottoproblemi; se _disgiunti_, puoi sommare le euristiche.
- **Apprendimento:** l‚Äôeuristica pu√≤ essere appresa automaticamente dai dati (ma non sempre ammissibile).
- **Combinazione lineare:** somma pesata di pi√π euristiche, con coefficienti scelti o appresi.

## Agenti classici
Gli agenti classici assumono che l'ambiente sia:
- **Completamente Osservabile:**¬†L'agente conosce tutto ci√≤ che √® rilevante per la sua decisione in ogni momento. Non ci sono informazioni nascoste.
- **Deterministico:**¬†Ogni azione ha un unico risultato, certo e prevedibile. Se un robot decide di andare avanti, andr√† avanti, senza possibilit√† di scivolare o deviare.
**La conseguenza di queste due assunzioni √® potentissima:**
- **Pianificazione Offline:**¬†L'agente pu√≤ calcolare l'intera sequenza di azioni per raggiungere l'obiettivo prima ancora di muovere un solo passo.
- **Esecuzione senza sorprese:**¬†Una volta creato, il piano pu√≤ essere eseguito a occhi chiusi, perch√© si √® certi che il mondo non cambier√† in modi imprevisti.
### RICERCA LOCALE
il **mondo reale √® raramente** cos√¨ semplice e prevedibile. Questo ci spinge a riconsiderare le nostre assunzioni e a cercare approcci pi√π flessibili e realistici.
Mentre gli algoritmi classici cercano un¬†**cammino soluzione**¬†(una sequenza di azioni per arrivare al goal),
- Si usa quando interessa **solo lo stato finale**, non il cammino
##### Come Funziona la Ricerca Locale?
Le sue caratteristiche principali sono:
- **Non √® sistematica**: 
	- Non garantisce di esplorare tutte le possibilit√†.
- **Mantiene solo lo stato corrente**: 
	- A differenza degli algoritmi classici che memorizzano un'intera frontiera di nodi, la ricerca locale tiene traccia solo della posizione attuale (il¬†**nodo corrente**).
- **Si muove tra nodi adiacenti**: 
	- Ad ogni passo, valuta gli stati vicini e si sposta in uno di essi, sperando di migliorare la situazione.
- **Memoria super efficiente**: 
	- Non tenendo traccia dei cammini passati, consuma una quantit√† di memoria minima e costante.
- **Ideale per problemi di ottimizzazione**: 
	- √à perfetta per trovare lo stato "migliore" secondo una¬†**funzione obiettivo**¬†(che vogliamo massimizzare) o lo stato a¬†**costo minore**¬†(che vogliamo minimizzare).

Per comprendere il funzionamento della **ricerca locale**, possiamo immaginare lo **spazio degli stati** come un vero e proprio **paesaggio tridimensionale** dove:
- ogni **punto** del paesaggio rappresenta **uno stato possibile** del problema;
- la **quota (altezza)** del punto rappresenta il **valore della funzione obiettivo**, cio√® quanto quello stato √® buono o conveniente.

L'obiettivo finale √® trovare il punto migliore dell'intera mappa.
- **Ottimo Globale (global maximum)**: √à il picco pi√π alto in assoluto, ovvero la¬†**migliore soluzione possibile**.
Tuttavia, il paesaggio √® complesso e pieno di "trappole" che possono ingannare un algoritmo semplice.
- **Ottimo Locale (local maximum) ‚õ∞Ô∏è**:
    - **Cos'√®**: Un picco che √® pi√π alto di tutti i suoi vicini, ma¬†**non √® il picco pi√π alto**¬†dell'intera mappa.
- **Altopiano (shoulder¬†o¬†plateau) üèúÔ∏è**:
    - **Cos'√®**: Una zona piatta dove tutti i vicini hanno la stessa altezza.

![Pasted image 20251028154347.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251028154347.png)

#### ALGORITMO HILL CLIMBING
L‚Äô**Hill Climbing** √® l‚Äôalgoritmo di **ricerca locale** pi√π semplice e intuitivo.  
a ogni passo, sceglie la direzione che porta **pi√π in alto possibile**, cio√® verso uno stato migliore, ma **senza pianificare a lungo termine**.
üëâ √à quindi un approccio **greedy (ingordo)** ‚Äî sceglie sempre la soluzione migliore **nell‚Äôimmediato**, senza preoccuparsi del futuro.
1. **Inizializzazione** ‚Äì si parte da uno stato casuale (stato corrente).
2. **Generazione** ‚Äì si producono tutti gli stati vicini (successori).
3. **Valutazione** ‚Äì si valuta la funzione obiettivo di ogni successore.
4. **Spostamento** ‚Äì si passa al vicino con valore migliore.
5. **Iterazione** ‚Äì il nuovo stato diventa corrente e il ciclo ricomincia.
üß† L‚Äôalgoritmo **non memorizza** gli stati passati

![Pasted image 20251028160108.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251028160108.png)

#### üîÅ Varianti Principali

|Variante|Descrizione|Vantaggi|
|---|---|---|
|**Steepest-Ascent (salita pi√π ripida)**|Sceglie il **miglior vicino in assoluto**|Converge rapidamente|
|**Stocastico**|Sceglie **a caso** tra i vicini migliori|Evita alcuni massimi locali|
|**Prima Scelta**|Genera vicini casualmente e accetta il **primo migliore**|Molto efficiente quando i vicini sono numerosi|
|**Con Mosse Laterali**|Consente passi con **valore uguale** al corrente per uscire da plateau|Evita blocchi su superfici piatte|
|**Con Riavvio Casuale**|Se si blocca, **riparte da un nuovo stato casuale**|Completo con probabilit√† 1 (prima o poi trova il massimo globale)|
#### ‚ö†Ô∏è Problemi Tipici: Le ‚ÄúTrappole‚Äù del Paesaggio

| Tipo di trappola            | Descrizione                                                | Effetto sull‚Äôalgoritmo                                  |
| --------------------------- | ---------------------------------------------------------- | ------------------------------------------------------- |
| **Massimo Locale(collina)** | Punto pi√π alto dei vicini, ma inferiore al massimo globale | L‚Äôagente si ferma troppo presto                         |
| **Altipiani (Plateau)**     | Area piatta dove tutti i vicini hanno lo stesso valore     | L‚Äôagente si muove a caso o si blocca                    |
| **Crinali (Ridge)**         | Serie di massimi locali separati da discese laterali       | L‚Äôagente non riesce a ‚Äúgirare‚Äù verso la salita corretta |
![Pasted image 20251028162118.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251028162118.png)
![Pasted image 20251028162130.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251028162130.png)
![Pasted image 20251028162157.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251028162157.png)

üìâ In questi casi, l‚Äôalgoritmo pu√≤ **fermarsi prematuramente** o ‚Äúgirare in tondo‚Äù senza mai raggiungere la soluzione ottimale.

#### ALGORITMO SIMULATED ANNEALING
√à una **combinazione tra:**
- **Hill Climbing**, che tende sempre a migliorare lo stato (approccio _greedy_),
- e una **scelta stocastica controllata**, che _a volte accetta stati peggiori_.
üëâ Questo serve a **sfuggire dai massimi locali**:  
accettando temporaneamente un peggioramento, l‚Äôalgoritmo pu√≤ ‚Äúscendere‚Äù da una collina minore per poi risalire verso un picco pi√π alto (il **massimo globale**).
La **temperatura** regola quanto l‚Äôalgoritmo √® disposto ad accettare peggioramenti:
- **Alta `T`** ‚Üí accetta spesso anche mosse peggiori ‚Üí ampia esplorazione;
- **Bassa `T`** ‚Üí accetta solo miglioramenti ‚Üí comportamento simile a Hill Climbing.
Man mano che il processo procede, la temperatura **decresce gradualmente** secondo un _cooling schedule_, ad esempio:
$$T_{k+1} = \alpha \cdot T_k \quad \text{con } 0<Œ±<1$$
### Funzionamento dell‚Äôalgoritmo
1. **Inizializzazione:** scegli uno stato iniziale casuale e imposta una temperatura iniziale `T‚ÇÄ`.
2. **Genera un successore casuale** dello stato corrente.
3. **Valuta la differenza di energia (o costo):**
    $\Delta E = f(n') - f(n)$
4. **Decidi se accettarlo:**
    - Se la mossa **migliora** la soluzione ($ŒîE < 0$) ‚Üí **accettala sempre**.
    - Se **peggiora** la soluzione ($ŒîE > 0$) ‚Üí **accettala con probabilit√†**
        $p = e^{-\Delta E/T}$
        (poich√© $ŒîE > 0$, il valore √® compreso tra 0 e 1).  
        In pratica:
        - pi√π la mossa √® ‚Äúpoco peggiore‚Äù ‚Üí pi√π probabile accettarla;
        - pi√π √® ‚Äúmolto peggiore‚Äù ‚Üí meno probabile.  
            L‚Äôalgoritmo genera un numero casuale in$[0,1]$ e accetta la mossa se √® $< p$.
5. **Aggiorna la temperatura:** riduci `T` secondo il piano di raffreddamento.
6. **Ripeti** finch√© `T` √® prossima a 0. 
### Algoritmo
![Pasted image 20251029193033.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251029193033.png)

####  Ricerca Local Beam
√à una ricerca locale che mantiene **k stati alla volta** (anzich√© uno solo, come nell‚ÄôHill Climbing).  
A ogni passo:
1. genera tutti i successori dei `k` stati;
2. se uno √® il goal, termina;
3. altrimenti seleziona i **k migliori** e ripete.
üëâ A differenza del riavvio casuale, le ricerche **collaborano**, condividendo i migliori risultati.  
‚ö†Ô∏è Rischia di perdere **diversit√†** (tutti i cammini si concentrano nella stessa zona).  
üé≤ Variante: **Beam stocastica**, che sceglie i successori con probabilit√† proporzionale al valore ‚Üí pi√π esplorazione.
### ALBERI AND-OR
Quando ci troviamo in **ambienti non deterministici**, l‚Äôalbero di ricerca classico (quello usato negli ambienti deterministici) **non basta pi√π**.  
- si utilizza una struttura chiamata **albero AND‚ÄìOR**
![Pasted image 20251029193155.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251029193155.png)
- NODI OR -> cerchio singolo
- NODI AND -> cerchio singolo + semicerchio
- Ogni **foglia** del sottoalbero √® un **nodo obiettivo**.
### Pseudocodice
![Pasted image 20251029193206.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251029193206.png)
#### Gestione dei cicli
- immaginando una situazione non deterministica 
	- il robot casualmente potrebbe fallire nelle sue azioni
- si pu√≤ incorrere in situazioni cicliche 
	- Esiste per√≤ una **soluzione ciclica**, che consiste nel **ripetere un‚Äôazione finch√© non riesce**.  
- Per esempio:
> ‚ÄúContinua a provare **Destra** finch√© non ti sposti davvero a destra‚Äù.
```scss
while (non sei nel riquadro destro)
    esegui Destra
```
![Pasted image 20251029193219.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251029193219.png)
#### Ricerca con Osservazioni Parziali
Quando l‚Äôagente si trova in un ambiente **parzialmente osservabile**, le sue percezioni **non bastano per sapere con certezza in quale stato si trova**.  
Deve quindi **gestire l‚Äôincertezza**, mantenendo una **rappresentazione interna** di ci√≤ che _potrebbe_ essere vero.
- perci√≤ ora l'obiettivo √® quello di raccogliere pi√π informazioni possibili

#### Ricerca sensor-less
- Se l‚Äôagente **non riceve alcuna informazione** durante l‚Äôesecuzione ‚Üí si parla di **problema senza sensori** o **problema conformante**.
	- La ricerca non avviene pi√π sugli **stati fisici**, ma sugli **stati-credenza**
	- Ogni stato-credenza rappresenta **tutte le situazioni fisiche possibili** in cui l‚Äôagente potrebbe trovarsi.
- Questo rende il problema **completamente osservabile** nel nuovo spazio, perch√© l‚Äôagente conosce sempre il proprio stato-credenza.
	- La ricerca non avviene pi√π sugli **stati fisici**, ma sugli **stati-credenza**, cio√® insiemi di stati.
**Componenti principali:**

|Elemento|Descrizione|
|---|---|
|**Stati**|Ogni belief state √® un sottoinsieme degli stati fisici possibili (fino a 2‚Åø).|
|**Stato iniziale**|Insieme di tutti gli stati compatibili con le informazioni note.|
|**Azioni**|Ammissibili se sicure in tutti gli stati del belief state.|
|**Transizioni**|Il nuovo belief state √® l‚Äôunione dei risultati dell‚Äôazione su ciascuno stato possibile.|
|**Test obiettivo**|√à raggiunto se almeno uno stato del belief state soddisfa l‚Äôobiettivo.|
|**Costo**|Pu√≤ essere medio o prudente (es. il massimo).|

###### Come vengono aggiornati gli stati credenza (versione deterministica e non)
![Pasted image 20251029193235.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251029193235.png)
###### Spazio degli stati completo
![Pasted image 20251029193247.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251029193247.png)
Quando l‚Äôagente agisce in un ambiente parzialmente osservabile, il passaggio tra stati-credenza avviene in **tre fasi principali**:

1. **üî∏ Predizione**
    - Si calcola lo **stato-credenza previsto** dopo aver eseguito un‚Äôazione, esattamente come nel caso senza sensori.
    - In questa fase si considerano **tutti i possibili risultati** dell‚Äôazione.
2. **üî∏ Percezioni possibili**
    - Dallo stato-credenza previsto si determinano **tutte le percezioni** che l‚Äôagente _potrebbe_ ricevere in base ai sensori.
    - Serve per stimare **quali informazioni** saranno disponibili dopo l‚Äôazione.
3. **üî∏ Aggiornamento (filtraggio)**
    - Per ogni percezione ricevuta, si costruisce un **nuovo stato-credenza** che include **solo gli stati compatibili** con quella percezione.
    - Tutti gli stati non coerenti vengono eliminati ‚Üí l‚Äôagente **riduce l‚Äôincertezza**.
#### Agenti per ricerca online
Finora abbiamo visto **ricerche offline**, dove l‚Äôagente pianifica **tutto il percorso prima di agire**.  
Negli ambienti **reali, dinamici o ignoti**, questo non √® sempre possibile: perci√≤ si usa la **ricerca online**, che alterna **azione ‚Üí osservazione ‚Üí decisione**.
- nella ricerca online l‚Äôagente **non calcola un piano completo**, ma decide passo dopo passo:
1. **Agisce** ‚Üí esegue un‚Äôazione.
2. **Osserva** ‚Üí percepisce il nuovo stato.
3. **Aggiorna** ‚Üí sceglie la prossima mossa in base a ci√≤ che ha appreso.
üìç √à utile in:
- **ambienti dinamici** (che cambiano nel tempo);
- **ambienti non deterministici** (le azioni non hanno sempre lo stesso effetto);
- **ambienti sconosciuti**, dove deve imparare gli effetti delle proprie azioni.
#### Ambienti sconosciuti
In un ambiente ignoto, l‚Äôagente **non conosce**:
- gli stati,
- gli effetti delle azioni,
- n√© i costi associati.
Deve **sperimentare** e costruire **un modello dell‚Äôambiente** (es. costruzione di mappe).  
Ogni azione serve sia per agire sia per **imparare**.
####  Problemi di ricerca online
Un problema di ricerca online coinvolge tre attivit√†: **elaborazione, percezione e azione**.
L‚Äôagente:
- conosce **solo lo stato attuale**;
- scopre **Risultato(s, a)** e **costo c(s,a,s‚Ä≤)** solo dopo aver agito;
- pu√≤ usare una **funzione euristica h(s)** come stima della distanza dal goal.
###### Obiettivo: raggiungere l‚Äôobiettivo **minimizzando il costo totale effettivo**.  
Il **rapporto di competitivit√†** confronta questo costo con quello della soluzione ottima in un ambiente noto (pi√π √® basso, meglio √®).
‚ö†Ô∏è **Limiti**:
- rischio di **vicoli ciechi** (stati da cui non si pu√≤ pi√π uscire);
- **nessun algoritmo** pu√≤ evitarli in tutti i casi;
- ambienti esplorabili in sicurezza solo se **non esistono azioni irreversibili**.
####  Agenti Online
Gli agenti **non pianificano**, ma decidono **una sola azione alla volta**.  
Devono ricordare le informazioni acquisite ‚Üí usano **backtracking** per tornare indietro.
#### Ricerca Locale Online
Come nella ricerca Hill Climbing:
- l‚Äôagente conosce `h(s)` solo dopo aver esplorato `s`;
- mantiene **un solo stato in memoria** ‚Üí √® gi√† un algoritmo **online**.
‚ö†Ô∏è Tuttavia, pu√≤ bloccarsi in un massimo locale e **non pu√≤ usare riavvii casuali**, perch√© non pu√≤ ‚Äúteletrasportarsi‚Äù.
#### Strategie Alternative
1. **Random Walk** ‚Üí l‚Äôagente sceglie casualmente una delle azioni possibili per uscire da massimi locali.
2. **LRTA*** (_Learning Real-Time A*_) ‚Üí l‚Äôagente impara durante la ricerca, aggiornando la propria euristica.
#### üí° LRTA* ‚Äî Learning Real-Time A*
Algoritmo che simula A*, ma **in tempo reale** e **localmente**:
- aggiorna il valore `H(s)` dello stato appena lasciato;
- poi sceglie la **mossa migliore** in base alle nuove stime.

![Pasted image 20251030112816.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251030112816.png)
![Pasted image 20251030112825.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251030112825.png)
###### üìä Propriet√†

|Propriet√†|Descrizione|
|---|---|
|**Completezza**|S√¨, in spazi esplorabili in sicurezza.|
|**Efficienza**|Nel caso peggiore visita ogni stato 2 volte, ma mediamente √® pi√π rapido della profondit√† online.|
|**Ottimalit√†**|Non garantita, salvo euristica perfetta.|
### Agenti basati sulla conoscenza

Gli **agenti basati sulla conoscenza (Knowledge-Based Agents)** sono sistemi che **ragionano sul mondo attraverso formule logiche**.  
L‚Äôelemento fondamentale √® la **Base di Conoscenza (KB)**, cio√® un insieme di **fatti e regole** che descrivono il mondo in modo simbolico.
La **KB** contiene un insieme di **formule logiche** (proposizioni o predicati) che rappresentano asserzioni sul mondo.
- Quando una formula √® accettata come vera **senza essere derivata**, si chiama **assioma**.
- Le formule possono essere **aggiunte**, **rimosse** o **interrogate** attraverso operazioni logiche:
	- **TELL(KB, œÜ)** ‚Üí inserisce nella KB una nuova formula (nuovo fatto o regola).
	- **ASK(KB, Œ±)** ‚Üí interroga la KB per verificare se Œ± √® una **conseguenza logica** delle informazioni memorizzate.
	- **RETRACT(KB, œÜ)** ‚Üí facoltativo, rimuove una formula
	- dove:
	-  **Œ±** = singolo fatto (formula)
	- **Œ¶** = insieme di fatti e regole (base di conoscenza)
![Pasted image 20251112115048.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251112115048.png)
#### Requisito fondamentale
> Ogni risposta dell‚Äôagente deve essere **una conseguenza logica** di ci√≤ che gli √® stato detto in precedenza.  
> In simboli: se **KB ‚ä® Œ±**, allora Œ± √® logicamente conseguente da KB.
#### PSEUDOCODICE DEL PROGRAMMA AGENTE CON KB
Un agente di questo tipo alterna **percezione, inferenza e azione**, aggiornando continuamente la KB.
![Pasted image 20251112114302.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251112114302.png)
L‚Äôagente quindi:
1. **Osserva** l‚Äôambiente (percezioni ‚Üí formule logiche);
2. **Ragiona** deducendo nuove informazioni (inferenza logica);
3. **Agisce** nel mondo e aggiorna la KB.
un agente basato su conoscenza pu√≤ essere
- **Dichiarativo:** si ‚Äúdice‚Äù all‚Äôagente _cosa sapere_ (formule logiche esplicite).  
    ‚ûú pi√π **modulare**, **manutenibile**, **spiegabile**.
- **Procedurale:** si ‚Äúprogramma‚Äù direttamente il comportamento con codice.  
    ‚ûú meno flessibile e pi√π difficile da modificare.
#### Componenti fondamentali della rappresentazione logica
- **Sintassi**
	- Definisce i simboli e le regole per costruire frasi logiche.
- **Semantica**
	- Stabilisce la corrispondenza tra formule e fatti del mondo (quando una formula √® vera).
- **Inferenza**
	- Insieme di regole che permettono di derivare nuove formule vere da quelle note.

> Una KB pu√≤ essere vista come l‚Äôinsieme di formule, oppure come una singola formula che le implica tutte.
#### Grounding (Radicamento)
- √à un processo
√à il legame tra **rappresentazione logica** e **mondo reale**.
- Le **percezioni sensoriali** producono formule vere nella KB (es. ‚ÄúOdore percepito ‚Üí Odore(2,3)‚Äù).
- Le **regole generali** derivano da **apprendimento induttivo**, che pu√≤ essere fallibile.
> In sostanza, il grounding collega le **formule** (mondo simbolico) con **gli stati reali del mondo** (mondo fisico).
### Rappresentazione e mondo
- La **rappresentazione logica** produce **nuovi fatti** (inferenze) coerenti con la realt√†.
- La **semantica** collega formule e mondo:
    - ‚Äúverso il basso‚Äù ‚Üí formule ‚Üí fatti reali (interpretazione);
    - ‚Äúverso l‚Äôalto‚Äù ‚Üí nuovi fatti logici ‚Üí nuovi aspetti veri del mondo.
![Pasted image 20251112112059.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251112112059.png)

Un‚Äô**interpretazione** I stabilisce la corrispondenza tra simboli e elementi reali.  
Una formula **A** √® **conseguenza logica** di KB se:  
$$KB ‚ä® A \ \text{‚áî}\ M(KB) ‚äÜ M(A)$$
cio√®: tutti i modelli che rendono vera KB rendono vera anche A.
- Una KB √® un insieme di formule logiche:
-  per M(qualcosa) si intende **l'insieme di tutti i modelli** (cio√® le interpretazioni del mondo) in cui **tutte le formule di quel qualcosa sono vere**.
>[!tip] per capire meglio
>Ora passiamo alla semantica:
>
> - $M(p)$ = tutti i mondi in cui **p √® vero**
>     
> - $M(q)$= tutti i mondi in cui **q √® vero**
>     
> - $M(KB) = M(p \land q) = M(p) \cap M(q)$
> Cio√®: l‚Äôinsieme dei mondi che rendono **vera la KB** √® l‚Äôintersezione dei mondi che rendono veri **tutti** i fatti in KB.
> üëâ Quindi **pi√π formule ci sono in KB, meno modelli soddisfano tutto**.  
> KB pi√π grande ‚Üí M(KB) **pi√π piccolo**.


![Pasted image 20251112124104.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251112124104.png)

### Ragionamento non monotono
Nella **logica classica** vale la **monotonia**: se $KB ‚ä® Œ±$, allora anche $KB ‚à™ \{Œ≤\} ‚ä® Œ±$ 
- Se aggiungo una nuova informazione non modifico ci√≤ che prima era vero
Nel ragionamento umano, invece, **nuove informazioni possono invalidare** conclusioni precedenti ‚Üí **ragionamento non monot√≤no**.

> Esempio: ‚ÄúGli uccelli volano; Tweety √® un uccello ‚üπ vola.‚Äù  
> Aggiungo: ‚ÄúTweety √® un pinguino‚Äù ‚üπ la conclusione non vale pi√π.  
> √à tipico del **ragionamento per default** e dell‚Äô**assunzione di mondo chiuso**.


[[ANNO 3/INTELLIGENZA ARTIFICIALE/SECONDO ESONERO/IA LEZ.7  DI LOGICA\|IA LEZ.7  DI LOGICA]]


### SPIEGAZIONE WUMPUS, 8 REGINE E GIOCO DELL'8
#### AGENTI REATTIVI=WUMPUS
#### 8 regine
![Pasted image 20251116165303.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251116165303.png)
Mettere **8 regine** su una scacchiera **8√ó8** in modo che **nessuna minacci un‚Äôaltra**.
Le regine attaccano:
- **sulla stessa riga**
- **sulla stessa colonna**
- **sulle diagonali**
##### Si usa l'algoritmo di hill climbing
- limitando il numero di mosse laterali(scegliere sempre la colonna pi√π a dx libera )
	- l'algoritmo delle 8 regine con hill climbing ha successo il 94% dei casi
	- e impiega 21 passi
	- possiamo avere meno iterazioni con il riavvio casuale
		- con 1/p riavvii
- senza hill climbing ci vorrebbero 
	- $1,8*10^{14}$ passi
#### gioco dell'8
- Hai una griglia **3√ó3** che contiene 8 tessere numerate e **una casella vuota** (‚Äúblank‚Äù).
- L‚Äôobiettivo √® **raggiungere lo stato finale** (goal state) facendo scorrere le tessere nella casella vuota.
![Pasted image 20251116172254.png](/img/user/ANNO%203/INTELLIGENZA%20ARTIFICIALE/IA%20FOTO/Pasted%20image%2020251116172254.png)

viene usato A*
A* genera tutti gli stati successori (le mosse possibili) e calcola:

$f(n) = g(n) + h(n)$

Poi sceglie di espandere **prima** lo stato con il f pi√π basso.

üëâ **Quindi la euristica serve solo per guidare la ricerca verso stati promettenti.**  
Non dice cosa fare, ma dice quali stati ‚Äúsembrano migliori‚Äù.
